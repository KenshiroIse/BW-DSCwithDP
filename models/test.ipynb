{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# https://www.youtube.com/watch?v=fR_0o25kigM&list=WL&index=6&t=2070s&ab_channel=AladdinPersson\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import ceil\n",
    "from torchinfo import summary\n",
    "import quant_module as qm\n",
    "# import quant_module as qm\n",
    "\n",
    "__all__ = ['mixeffnet_b0_w1234a234', 'mixeffnet_b0_w1234a234_100', 'mixeffnet_b0_w248a248_chan',\"mixeffnet_b0_w2468a2468_100\",\n",
    "           'mixeffnet_b3_w2468a2468_100']\n",
    "\n",
    "base_model = [\n",
    "    # expand_ratio, channels, repeats, stride, kernel_size\n",
    "    [1, 16, 1, 1, 3],\n",
    "    [6, 24, 2, 2, 3],\n",
    "    [6, 40, 2, 2, 5],\n",
    "    [6, 80, 3, 2, 3],\n",
    "    [6, 112, 3, 1, 5],\n",
    "    [6, 192, 4, 2, 5],\n",
    "    [6, 320, 1, 1, 3],\n",
    "]\n",
    "\n",
    "phi_values = {\n",
    "    # tuple of: (phi_value, resolution, drop_rate)\n",
    "    \"b0\": (0, 224, 0.2),  # alpha, beta, gamma, depth = alpha ** phi\n",
    "    \"b1\": (0.5, 240, 0.2),\n",
    "    \"b2\": (1, 260, 0.3),\n",
    "    \"b3\": (2, 300, 0.3),\n",
    "    \"b4\": (3, 380, 0.4),\n",
    "    \"b5\": (4, 456, 0.4),\n",
    "    \"b6\": (5, 528, 0.5),\n",
    "    \"b7\": (6, 600, 0.5),\n",
    "}\n",
    "\n",
    "\n",
    "class BasicCNNBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicCNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "        self.silu = nn.SiLU()  # SiLU <-> Swish\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return self.silu(x)\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, conv_func, in_channels, out_channels, \n",
    "        kernel_size, stride, padding, groups=1, **kwargs\n",
    "    ):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.cnn = conv_func(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.silu = nn.SiLU()  # SiLU <-> Swish\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = self.silu(self.bn(out))\n",
    "        return out\n",
    "    # def forward(self, x):\n",
    "    #     return self.silu(self.bn(self.cnn(x)))\n",
    "\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, conv_func, in_channels, reduced_dim, **kwargs):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),  # C x H x W -> C x 1 x 1\n",
    "            conv_func(\n",
    "                in_channels, reduced_dim, kernel_size=1, bias=False, **kwargs\n",
    "            ),  # C x 1 x 1 -> C_reduced x 1 x 1\n",
    "            nn.SiLU(),  # SiLU <-> Swish\n",
    "            conv_func(\n",
    "                reduced_dim, in_channels, kernel_size=1, bias=False, **kwargs\n",
    "            ),  # C_reduced x 1 x 1 -> C x 1 x 1\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "\n",
    "\n",
    "class InvertedResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            conv_func,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            expand_ratio,\n",
    "            reduction=4,  # squeeze excitation\n",
    "            survival_prob=0.8,  # for stochastic depth\n",
    "             **kwargs,\n",
    "    ):\n",
    "        super(InvertedResidualBlock, self).__init__()\n",
    "        self.survival_prob = survival_prob\n",
    "        self.use_residual = in_channels == out_channels and stride == 1\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.expand = in_channels != hidden_dim\n",
    "        reduced_dim = int(in_channels / reduction)\n",
    "\n",
    "        if self.expand:\n",
    "            self.expand_conv = CNNBlock(\n",
    "                conv_func, in_channels, hidden_dim, kernel_size=3, stride=1, padding=1, **kwargs,\n",
    "            )\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            CNNBlock(\n",
    "                conv_func, hidden_dim, hidden_dim, kernel_size, stride, padding, groups=hidden_dim, **kwargs,\n",
    "            ),\n",
    "            SqueezeExcitation(conv_func, hidden_dim, reduced_dim, **kwargs,),\n",
    "            conv_func(hidden_dim, out_channels, kernel_size=1, bias=False, **kwargs,),\n",
    "            nn.BatchNorm2d(out_channels,),\n",
    "        )\n",
    "\n",
    "    def stochastic_depth(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "\n",
    "        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.survival_prob\n",
    "        return torch.div(x, self.survival_prob) * binary_tensor\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.expand_conv(inputs) if self.expand else inputs\n",
    "        \n",
    "        if self.use_residual:\n",
    "            # return self.stochastic_depth(self.conv(x)) + inputs\n",
    "            if self.expand:\n",
    "                return self.stochastic_depth(self.conv(x)) + self.expand_conv.cnn.quant_skip\n",
    "            else:\n",
    "                return self.stochastic_depth(self.conv(x)) + self.conv[0].cnn.quant_skip\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_func, version, num_classes=1000, **kwargs):\n",
    "        if 'abits' in kwargs:\n",
    "            print('abits: {}'.format(kwargs['abits']))\n",
    "        if 'wbits' in kwargs:\n",
    "            print('wbits: {}'.format(kwargs['wbits']))\n",
    "        self.conv_func = conv_func\n",
    "        super(EfficientNet, self).__init__()\n",
    "        width_factor, depth_factor, dropout_rate, res = self.calculate_factors(version)\n",
    "        self.res = res\n",
    "        last_channel = ceil(1280 * width_factor)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.features = self.create_features(conv_func, width_factor, depth_factor, last_channel, **kwargs)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "    def calculate_factors(self, version, alpha=1.2, beta=1.1):\n",
    "        phi, res, drop_rate = phi_values[version]\n",
    "        depth_factor = alpha ** phi\n",
    "        width_factor = beta ** phi\n",
    "        return width_factor, depth_factor, drop_rate, res\n",
    "    \n",
    "    def create_features(self, conv_func, width_factor, depth_factor, last_channel, **kwargs):\n",
    "        channels = int(32 * width_factor)\n",
    "        features = [BasicCNNBlock(3, channels, kernel_size=3, stride=2, padding=1)]\n",
    "        in_channels = channels\n",
    "\n",
    "        for expand_ratio, channels, repeats, stride, kernel_size in base_model:\n",
    "            out_channels = 4 * ceil(int(channels * width_factor) / 4)\n",
    "            layers_repeats = ceil(repeats * depth_factor)\n",
    "\n",
    "            for layer in range(layers_repeats):\n",
    "                features.append(\n",
    "                    InvertedResidualBlock(\n",
    "                        conv_func,\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        expand_ratio=expand_ratio,\n",
    "                        stride=stride if layer == 0 else 1,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=kernel_size // 2,  # if k=1:pad=0, k=3:pad=1, k=5:pad=2\n",
    "                         **kwargs,\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "        features.append(\n",
    "            CNNBlock(conv_func, in_channels, last_channel, kernel_size=1, stride=1, padding=0,  **kwargs)\n",
    "        )\n",
    "        return nn.Sequential(*features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.features(x))\n",
    "        return self.classifier(x.view(x.shape[0], -1))\n",
    "    def complexity_loss(self):\n",
    "        size_product = []\n",
    "        loss = 0\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, self.conv_func):\n",
    "                complexity_loss, _ = m.complexity_loss()\n",
    "                loss += complexity_loss\n",
    "                size_product += [m.size_product]\n",
    "        normalizer = size_product[0].item()\n",
    "        loss /= normalizer\n",
    "        return loss\n",
    "    \n",
    "    def split_complexity_loss(self):\n",
    "        loss = 0\n",
    "        layer_idx = 0\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, self.conv_func):\n",
    "                _, split_complexity_loss = m.complexity_loss()\n",
    "                if layer_idx in [0, 4, 9, 19, 44, 59]:\n",
    "                    loss += split_complexity_loss\n",
    "                layer_idx += 1\n",
    "        normalizer = 6 * 5\n",
    "        loss /= normalizer\n",
    "        return loss\n",
    "    \n",
    "    def fetch_best_arch(self):\n",
    "        sum_bitops, sum_bita, sum_bitw = 0, 0, 0\n",
    "        sum_mixbitops, sum_mixbita, sum_mixbitw = 0, 0, 0\n",
    "        layer_idx = 0\n",
    "        best_arch = None\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, self.conv_func):\n",
    "                layer_arch, bitops, bita, bitw, mixbitops, mixbita, mixbitw = m.fetch_best_arch(layer_idx)\n",
    "                if best_arch is None:\n",
    "                    best_arch = layer_arch\n",
    "                else:\n",
    "                    for key in layer_arch.keys():\n",
    "                        if key not in best_arch:\n",
    "                            best_arch[key] = layer_arch[key]\n",
    "                        else:\n",
    "                            best_arch[key].append(layer_arch[key][0])\n",
    "                sum_bitops += bitops\n",
    "                sum_bita += bita\n",
    "                sum_bitw += bitw\n",
    "                sum_mixbitops += mixbitops\n",
    "                sum_mixbita += mixbita\n",
    "                sum_mixbitw += mixbitw\n",
    "                layer_idx += 1\n",
    "        return best_arch, sum_bitops, sum_bita, sum_bitw, sum_mixbitops, sum_mixbita, sum_mixbitw\n",
    "\n",
    "\n",
    "\n",
    "def mixeffnet_b0_w1234a234(**kwargs):\n",
    "    version = \"b0\"\n",
    "    return EfficientNet(qm.MixActivConv2d, version, num_classes=1000,\n",
    "                     wbits=[1, 2, 3, 4], abits=[2, 3, 4], share_weight=True, **kwargs)\n",
    "\n",
    "def mixeffnet_b0_w1234a234_100(**kwargs):\n",
    "    version = \"b0\"\n",
    "    return EfficientNet(qm.MixActivConv2d, version, num_classes=100,\n",
    "                     wbits=[1, 2, 3, 4], abits=[2, 3, 4], share_weight=True, **kwargs)\n",
    "    \n",
    "def mixeffnet_b0_w2468a2468_100(**kwargs):\n",
    "    version = \"b0\"\n",
    "    return EfficientNet(qm.MixActivConv2d, version, num_classes=100,\n",
    "                     wbits=[2, 4, 6, 8], abits=[2, 4, 6, 8], share_weight=True, **kwargs)\n",
    "\n",
    "def mixeffnet_b0_w248a248_chan(**kwargs):\n",
    "    version = \"b0\"\n",
    "    return EfficientNet(qm.MixActivChanConv2d, version, num_classes=100,\n",
    "                     wbits=[1, 2, 3, 4], abits=[2, 3, 4], share_weight=True, **kwargs)\n",
    "\n",
    "\n",
    "def mixeffnet_b3_w2468a2468_100(**kwargs):\n",
    "    version = \"b3\"\n",
    "    return EfficientNet(qm.MixActivConv2d, version, num_classes=100,\n",
    "                     wbits=[2, 4, 6, 8], abits=[2, 4, 6, 8], share_weight=True, **kwargs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # @classmethod\n",
    "    # def get_image_size(cls, model_name):\n",
    "    #     \"\"\"Get the input image size for a given efficientnet model.\n",
    "\n",
    "    #     Args:\n",
    "    #         model_name (str): Name for efficientnet.\n",
    "\n",
    "    #     Returns:\n",
    "    #         Input image size (resolution).\n",
    "    #     \"\"\"\n",
    "    #     cls._check_model_name_is_valid(model_name)\n",
    "    #     _, _, res, _ = efficientnet_params(model_name)\n",
    "    #     return res\n",
    "    \n",
    "    #image_size = EfficientNet.get_image_size(args.arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mobilenetv3:\n",
      " MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Hswish()\n",
      "    )\n",
      "    (1): MobileBottleneck(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
      "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): SEModule(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=16, out_features=4, bias=False)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=4, out_features=16, bias=False)\n",
      "            (3): Hsigmoid()\n",
      "          )\n",
      "        )\n",
      "        (6): ReLU(inplace=True)\n",
      "        (7): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): MobileBottleneck(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "        (4): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Identity()\n",
      "        (6): ReLU(inplace=True)\n",
      "        (7): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (8): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): MobileBottleneck(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
      "        (4): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): Identity()\n",
      "        (6): ReLU(inplace=True)\n",
      "        (7): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (8): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): MobileBottleneck(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hswish()\n",
      "        (3): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): SEModule(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=96, out_features=24, bias=False)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=24, out_features=96, bias=False)\n",
      "            (3): Hsigmoid()\n",
      "          )\n",
      "        )\n",
      "        (6): Hswish()\n",
      "        (7): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (8): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): MobileBottleneck(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hswish()\n",
      "        (3): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "        (4): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): SEModule(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=240, out_features=60, bias=False)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=60, out_features=240, bias=False)\n",
      "            (3): Hsigmoid()\n",
      "          )\n",
      "        )\n",
      "        (6): Hswish()\n",
      "        (7): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (8): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): MobileBottleneck(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hswish()\n",
      "        (3): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "        (4): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): SEModule(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=240, out_features=60, bias=False)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=60, out_features=240, bias=False)\n",
      "            (3): Hsigmoid()\n",
      "          )\n",
      "        )\n",
      "        (6): Hswish()\n",
      "        (7): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (8): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): MobileBottleneck(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hswish()\n",
      "        (3): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "        (4): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): SEModule(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=120, out_features=30, bias=False)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=30, out_features=120, bias=False)\n",
      "            (3): Hsigmoid()\n",
      "          )\n",
      "        )\n",
      "        (6): Hswish()\n",
      "        (7): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (8): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): MobileBottleneck(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hswish()\n",
      "        (3): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): SEModule(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=144, out_features=36, bias=False)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=36, out_features=144, bias=False)\n",
      "            (3): Hsigmoid()\n",
      "          )\n",
      "        )\n",
      "        (6): Hswish()\n",
      "        (7): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (8): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): MobileBottleneck(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hswish()\n",
      "        (3): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
      "        (4): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): SEModule(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=288, out_features=72, bias=False)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=72, out_features=288, bias=False)\n",
      "            (3): Hsigmoid()\n",
      "          )\n",
      "        )\n",
      "        (6): Hswish()\n",
      "        (7): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): MobileBottleneck(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hswish()\n",
      "        (3): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): SEModule(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=576, out_features=144, bias=False)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=144, out_features=576, bias=False)\n",
      "            (3): Hsigmoid()\n",
      "          )\n",
      "        )\n",
      "        (6): Hswish()\n",
      "        (7): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): MobileBottleneck(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): Hswish()\n",
      "        (3): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): SEModule(\n",
      "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc): Sequential(\n",
      "            (0): Linear(in_features=576, out_features=144, bias=False)\n",
      "            (1): ReLU(inplace=True)\n",
      "            (2): Linear(in_features=144, out_features=576, bias=False)\n",
      "            (3): Hsigmoid()\n",
      "          )\n",
      "        )\n",
      "        (6): Hswish()\n",
      "        (7): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Hswish()\n",
      "    )\n",
      "    (13): AdaptiveAvgPool2d(output_size=1)\n",
      "    (14): Conv2d(576, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (15): Hswish()\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.8, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 239\u001b[0m\n\u001b[1;32m    237\u001b[0m model \u001b[38;5;241m=\u001b[39m mobilenetv3()\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmobilenetv3:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, model)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotal params: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnet\u001b[49m\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000000.0\u001b[39m))\n\u001b[1;32m    240\u001b[0m input_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n\u001b[1;32m    243\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(input_size)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "# https://github.com/kuan-wang/pytorch-mobilenet-v3/tree/master\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "__all__ = ['MobileNetV3', 'mobilenetv3']\n",
    "\n",
    "\n",
    "def conv_bn(inp, oup, stride, conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, nlin_layer=nn.ReLU):\n",
    "    return nn.Sequential(\n",
    "        conv_layer(inp, oup, 3, stride, 1, bias=False),\n",
    "        norm_layer(oup),\n",
    "        nlin_layer(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup, conv_layer=nn.Conv2d, norm_layer=nn.BatchNorm2d, nlin_layer=nn.ReLU):\n",
    "    return nn.Sequential(\n",
    "        conv_layer(inp, oup, 1, 1, 0, bias=False),\n",
    "        norm_layer(oup),\n",
    "        nlin_layer(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "class Hswish(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(Hswish, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * F.relu6(x + 3., inplace=self.inplace) / 6.\n",
    "\n",
    "\n",
    "class Hsigmoid(nn.Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(Hsigmoid, self).__init__()\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu6(x + 3., inplace=self.inplace) / 6.\n",
    "\n",
    "\n",
    "class SEModule(nn.Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            Hsigmoid()\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_divisible(x, divisible_by=8):\n",
    "    import numpy as np\n",
    "    return int(np.ceil(x * 1. / divisible_by) * divisible_by)\n",
    "\n",
    "\n",
    "class MobileBottleneck(nn.Module):\n",
    "    def __init__(self, inp, oup, kernel, stride, exp, se=False, nl='RE'):\n",
    "        super(MobileBottleneck, self).__init__()\n",
    "        assert stride in [1, 2]\n",
    "        assert kernel in [3, 5]\n",
    "        padding = (kernel - 1) // 2\n",
    "        self.use_res_connect = stride == 1 and inp == oup\n",
    "\n",
    "        conv_layer = nn.Conv2d\n",
    "        norm_layer = nn.BatchNorm2d\n",
    "        if nl == 'RE':\n",
    "            nlin_layer = nn.ReLU # or ReLU6\n",
    "        elif nl == 'HS':\n",
    "            nlin_layer = Hswish\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        if se:\n",
    "            SELayer = SEModule\n",
    "        else:\n",
    "            SELayer = Identity\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # pw\n",
    "            conv_layer(inp, exp, 1, 1, 0, bias=False),\n",
    "            norm_layer(exp),\n",
    "            nlin_layer(inplace=True),\n",
    "            # dw\n",
    "            conv_layer(exp, exp, kernel, stride, padding, groups=exp, bias=False),\n",
    "            norm_layer(exp),\n",
    "            SELayer(exp),\n",
    "            nlin_layer(inplace=True),\n",
    "            # pw-linear\n",
    "            conv_layer(exp, oup, 1, 1, 0, bias=False),\n",
    "            norm_layer(oup),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, dropout=0.8, mode='small', width_mult=1.0):\n",
    "        super(MobileNetV3, self).__init__()\n",
    "        input_channel = 16\n",
    "        last_channel = 1280\n",
    "        if mode == 'large':\n",
    "            # refer to Table 1 in paper\n",
    "            mobile_setting = [\n",
    "                # k, exp, c,  se,     nl,  s,\n",
    "                [3, 16,  16,  False, 'RE', 1],\n",
    "                [3, 64,  24,  False, 'RE', 2],\n",
    "                [3, 72,  24,  False, 'RE', 1],\n",
    "                [5, 72,  40,  True,  'RE', 2],\n",
    "                [5, 120, 40,  True,  'RE', 1],\n",
    "                [5, 120, 40,  True,  'RE', 1],\n",
    "                [3, 240, 80,  False, 'HS', 2],\n",
    "                [3, 200, 80,  False, 'HS', 1],\n",
    "                [3, 184, 80,  False, 'HS', 1],\n",
    "                [3, 184, 80,  False, 'HS', 1],\n",
    "                [3, 480, 112, True,  'HS', 1],\n",
    "                [3, 672, 112, True,  'HS', 1],\n",
    "                [5, 672, 160, True,  'HS', 2],\n",
    "                [5, 960, 160, True,  'HS', 1],\n",
    "                [5, 960, 160, True,  'HS', 1],\n",
    "            ]\n",
    "        elif mode == 'small':\n",
    "            # refer to Table 2 in paper\n",
    "            mobile_setting = [\n",
    "                # k, exp, c,  se,     nl,  s,\n",
    "                [3, 16,  16,  True,  'RE', 2],\n",
    "                [3, 72,  24,  False, 'RE', 2],\n",
    "                [3, 88,  24,  False, 'RE', 1],\n",
    "                [5, 96,  40,  True,  'HS', 2],\n",
    "                [5, 240, 40,  True,  'HS', 1],\n",
    "                [5, 240, 40,  True,  'HS', 1],\n",
    "                [5, 120, 48,  True,  'HS', 1],\n",
    "                [5, 144, 48,  True,  'HS', 1],\n",
    "                [5, 288, 96,  True,  'HS', 2],\n",
    "                [5, 576, 96,  True,  'HS', 1],\n",
    "                [5, 576, 96,  True,  'HS', 1],\n",
    "            ]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        last_channel = make_divisible(last_channel * width_mult) if width_mult > 1.0 else last_channel\n",
    "        self.features = [conv_bn(3, input_channel, 2, nlin_layer=Hswish)]\n",
    "        self.classifier = []\n",
    "\n",
    "        # building mobile blocks\n",
    "        for k, exp, c, se, nl, s in mobile_setting:\n",
    "            output_channel = make_divisible(c * width_mult)\n",
    "            exp_channel = make_divisible(exp * width_mult)\n",
    "            self.features.append(MobileBottleneck(input_channel, output_channel, k, s, exp_channel, se, nl))\n",
    "            input_channel = output_channel\n",
    "\n",
    "        # building last several layers\n",
    "        if mode == 'large':\n",
    "            last_conv = make_divisible(960 * width_mult)\n",
    "            self.features.append(conv_1x1_bn(input_channel, last_conv, nlin_layer=Hswish))\n",
    "            self.features.append(nn.AdaptiveAvgPool2d(1))\n",
    "            self.features.append(nn.Conv2d(last_conv, last_channel, 1, 1, 0))\n",
    "            self.features.append(Hswish(inplace=True))\n",
    "        elif mode == 'small':\n",
    "            last_conv = make_divisible(576 * width_mult)\n",
    "            self.features.append(conv_1x1_bn(input_channel, last_conv, nlin_layer=Hswish))\n",
    "            # self.features.append(SEModule(last_conv))  # refer to paper Table2, but I think this is a mistake\n",
    "            self.features.append(nn.AdaptiveAvgPool2d(1))\n",
    "            self.features.append(nn.Conv2d(last_conv, last_channel, 1, 1, 0))\n",
    "            self.features.append(Hswish(inplace=True))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout),    # refer to paper section 6\n",
    "            nn.Linear(last_channel, n_class),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.mean(3).mean(2)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "def mobilenetv3(pretrained=False, **kwargs):\n",
    "    model = MobileNetV3(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = torch.load('mobilenetv3_small_67.4.pth.tar')\n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "        # raise NotImplementedError\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = mobilenetv3()\n",
    "    print('mobilenetv3:\\n', model)\n",
    "    print('Total params: %.2fM' % (sum(p.numel() for p in net.parameters())/1000000.0))\n",
    "    input_size=(1, 3, 224, 224)\n",
    "\n",
    "\n",
    "    x = torch.randn(input_size)\n",
    "    out = model(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abits: [2, 4, 6, 8]\n",
      "wbits: [2, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model = mixeffnet_b3_w2468a2468_100()\n",
    "x = summary(model=model, input_size=(1, 3, 300, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import ceil\n",
    "from torchinfo import summary\n",
    "import quant_module as qm\n",
    "# \n",
    "\n",
    "__all__ = ['quanteffnet_w8a8_chan', 'quanteffnet_cfg', \"quanteffnet_w32a32_chan\", \n",
    "           \"quanteffnet_w2a2_chan\", \"quanteffnet_w4a4_chan\", \"quanteffnet_w8a8\", \n",
    "           \"quanteffnet_cfg_2468\", \"quanteffnet_w4a4\", \"quanteffnet_w3a3\", \"quanteffnet_w2a2\",\n",
    "           \"quanteffnet_cfg_2468_b3\", \"quanteffnet_w8a8_b3\"]\n",
    "\n",
    "base_model = [\n",
    "    # expand_ratio, channels, repeats, stride, kernel_size\n",
    "    [1, 16, 1, 1, 3],\n",
    "    [6, 24, 2, 2, 3],\n",
    "    [6, 40, 2, 2, 5],\n",
    "    [6, 80, 3, 2, 3],\n",
    "    [6, 112, 3, 1, 5],\n",
    "    [6, 192, 4, 2, 5],\n",
    "    [6, 320, 1, 1, 3],\n",
    "]\n",
    "\n",
    "phi_values = {\n",
    "    # tuple of: (phi_value, resolution, drop_rate)\n",
    "    \"b0\": (0, 224, 0.2),  # alpha, beta, gamma, depth = alpha ** phi\n",
    "    \"b1\": (0.5, 240, 0.2),\n",
    "    \"b2\": (1, 260, 0.3),\n",
    "    \"b3\": (2, 300, 0.3),\n",
    "    \"b4\": (3, 380, 0.4),\n",
    "    \"b5\": (4, 456, 0.4),\n",
    "    \"b6\": (5, 528, 0.5),\n",
    "    \"b7\": (6, 600, 0.5),\n",
    "}\n",
    "\n",
    "\n",
    "class BasicCNNBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicCNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "        self.silu = nn.SiLU()  # SiLU <-> Swish\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return self.silu(x)\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, conv_func, in_channels, out_channels, wbit, abit,\n",
    "        kernel_size, stride, padding, groups=1, **kwargs\n",
    "    ):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.cnn = conv_func(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            wbit,\n",
    "            abit,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            groups=groups,\n",
    "            bias=False,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.silu = nn.SiLU()  # SiLU <-> Swish\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.silu(self.bn(self.cnn(x)))\n",
    "\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    def __init__(self, conv_func, archws, archas, in_channels, reduced_dim, **kwargs):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        assert len(archas) == 2\n",
    "        assert len(archws) == 2\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),  # C x H x W -> C x 1 x 1\n",
    "            conv_func(\n",
    "                in_channels, reduced_dim, archws[0], archas[0], kernel_size=1, bias=False, **kwargs\n",
    "            ),  # C x 1 x 1 -> C_reduced x 1 x 1\n",
    "            nn.SiLU(),  # SiLU <-> Swish\n",
    "            conv_func(\n",
    "                reduced_dim, in_channels, archws[1], archas[1], kernel_size=1, bias=False, **kwargs\n",
    "            ),  # C_reduced x 1 x 1 -> C x 1 x 1\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "\n",
    "\n",
    "class InvertedResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            conv_func,\n",
    "            archws, \n",
    "            archas,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size,\n",
    "            stride,\n",
    "            padding,\n",
    "            expand_ratio,\n",
    "            reduction=4,  # squeeze excitation\n",
    "            survival_prob=0.8,  # for stochastic depth\n",
    "             **kwargs,\n",
    "    ):\n",
    "        super(InvertedResidualBlock, self).__init__()\n",
    "\n",
    "        i = 0\n",
    "        self.survival_prob = survival_prob\n",
    "        self.use_residual = in_channels == out_channels and stride == 1\n",
    "        hidden_dim = in_channels * expand_ratio\n",
    "        self.expand = in_channels != hidden_dim\n",
    "        reduced_dim = int(in_channels / reduction)\n",
    "        if self.expand:\n",
    "            assert len(archas) == 5\n",
    "            assert len(archws) == 5\n",
    "        else:\n",
    "            assert len(archas) == 4\n",
    "            assert len(archws) == 4\n",
    "        if self.expand:\n",
    "            self.expand_conv = CNNBlock(\n",
    "                conv_func, in_channels, hidden_dim, archws[i], archas[i], kernel_size=3, stride=1, padding=1, **kwargs,\n",
    "            )\n",
    "            i += 1\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            CNNBlock(\n",
    "                conv_func, hidden_dim, hidden_dim, archws[i], archas[i], kernel_size, stride, padding, groups=hidden_dim, **kwargs,\n",
    "            ),\n",
    "            SqueezeExcitation(conv_func, archws[i+1:i+3], archas[i+1:i+3], hidden_dim, reduced_dim, **kwargs,),\n",
    "            conv_func(hidden_dim, out_channels, archws[i+3], archas[i+3], kernel_size=1, bias=False, **kwargs,),\n",
    "            nn.BatchNorm2d(out_channels,),\n",
    "        )\n",
    "\n",
    "    def stochastic_depth(self, x):\n",
    "        if not self.training:\n",
    "            return x\n",
    "\n",
    "        binary_tensor = torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.survival_prob\n",
    "        return torch.div(x, self.survival_prob) * binary_tensor\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.expand_conv(inputs) if self.expand else inputs\n",
    "\n",
    "        if self.use_residual:\n",
    "            # return self.stochastic_depth(self.conv(x)) + inputs\n",
    "            if self.expand:\n",
    "                return self.stochastic_depth(self.conv(x)) + self.expand_conv.cnn.quant_skip\n",
    "            else:\n",
    "                return self.stochastic_depth(self.conv(x)) + self.conv[0].cnn.quant_skip\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_func, version, archws, archas, num_classes=1000, **kwargs):\n",
    "        print('archas: {}'.format(archas))\n",
    "        print('archws: {}'.format(archws))\n",
    "        # assert len(archas) == 80\n",
    "        # assert len(archws) == 80\n",
    "        self.conv_func = conv_func\n",
    "        super(EfficientNet, self).__init__()\n",
    "        width_factor, depth_factor, dropout_rate, res = self.calculate_factors(version)\n",
    "        self.res = res\n",
    "        last_channel = ceil(1280 * width_factor)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.features = self.create_features(conv_func, width_factor, depth_factor, last_channel, archws, archas, **kwargs)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "    def calculate_factors(self, version, alpha=1.2, beta=1.1):\n",
    "        phi, res, drop_rate = phi_values[version]\n",
    "        depth_factor = alpha ** phi\n",
    "        width_factor = beta ** phi\n",
    "        return width_factor, depth_factor, drop_rate, res\n",
    "    \n",
    "    def create_features(self, conv_func, width_factor, depth_factor, last_channel, archws, archas, **kwargs):\n",
    "        channels = int(32 * width_factor)\n",
    "        features = [BasicCNNBlock(3, channels, kernel_size=3, stride=2, padding=1)]\n",
    "        in_channels = channels\n",
    "        i = 0\n",
    "        for expand_ratio, channels, repeats, stride, kernel_size in base_model:\n",
    "            out_channels = 4 * ceil(int(channels * width_factor) / 4)\n",
    "            layers_repeats = ceil(repeats * depth_factor)\n",
    "            if expand_ratio == 1:\n",
    "                j = 4\n",
    "            else:\n",
    "                j = 5\n",
    "            for layer in range(layers_repeats):\n",
    "                features.append(\n",
    "                    InvertedResidualBlock(\n",
    "                        conv_func,\n",
    "                        archws[i:i+j],\n",
    "                        archas[i:i+j],\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        expand_ratio=expand_ratio,\n",
    "                        stride=stride if layer == 0 else 1,\n",
    "                        kernel_size=kernel_size,\n",
    "                        padding=kernel_size // 2,  # if k=1:pad=0, k=3:pad=1, k=5:pad=2\n",
    "                         **kwargs,\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "                i += j\n",
    "        # assert i == 79\n",
    "        features.append(\n",
    "            CNNBlock(conv_func, in_channels, last_channel, archws[j], archas[j], kernel_size=1, stride=1, padding=0,  **kwargs)\n",
    "        )\n",
    "        return nn.Sequential(*features)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        return self.classifier(x.view(x.shape[0], -1))\n",
    "\n",
    "    def fetch_arch_info(self):\n",
    "        sum_bitops, sum_bita, sum_bitw = 0, 0, 0\n",
    "        layer_idx = 0\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, self.conv_func):\n",
    "                size_product = m.size_product.item()\n",
    "                memory_size = m.memory_size.item()\n",
    "                bitops = size_product * m.abit * m.wbit\n",
    "                bita = m.memory_size.item() * m.abit\n",
    "                bitw = m.param_size * m.wbit\n",
    "                # weight_shape = list(m.conv.weight.shape)\n",
    "                # print('idx {} with shape {}, bitops: {:.3f}M * {} * {}, memory: {:.3f}K * {}, '\n",
    "                #       'param: {:.3f}M * {}'.format(layer_idx, weight_shape, size_product, m.abit,\n",
    "                #                                    m.wbit, memory_size, m.abit, m.param_size, m.wbit))\n",
    "                sum_bitops += bitops\n",
    "                sum_bita += bita\n",
    "                sum_bitw += bitw\n",
    "                layer_idx += 1\n",
    "        return sum_bitops, sum_bita, sum_bitw\n",
    "\n",
    "\n",
    "def _load_arch(arch_path, names_nbits):\n",
    "    checkpoint = torch.load(arch_path)\n",
    "    state_dict = checkpoint['state_dict']\n",
    "    best_arch, worst_arch = {}, {}\n",
    "    for name in names_nbits.keys():\n",
    "        best_arch[name], worst_arch[name] = [], []\n",
    "    for name, params in state_dict.items():\n",
    "        name = name.split('.')[-1]\n",
    "        if name in names_nbits.keys():\n",
    "            alpha = params.cpu().numpy()\n",
    "            assert names_nbits[name] == alpha.shape[0]\n",
    "            best_arch[name].append(alpha.argmax())\n",
    "            worst_arch[name].append(alpha.argmin())\n",
    "\n",
    "    return best_arch, worst_arch\n",
    "    \n",
    "\n",
    "def quanteffnet_w8a8(arch_cfg_path, **kwargs):\n",
    "    version = \"b0\"\n",
    "    archas = [8] *80\n",
    "    archws = [8] *80\n",
    "    assert len(archas) == 80\n",
    "    assert len(archws) == 80\n",
    "    return EfficientNet(qm.QuantActivConv2d, version, archws, archas, num_classes=100, **kwargs)\n",
    "\n",
    "def quanteffnet_w4a4(arch_cfg_path, **kwargs):\n",
    "    version = \"b0\"\n",
    "    archas = [4] *80\n",
    "    archws = [4] *80\n",
    "    assert len(archas) == 80\n",
    "    assert len(archws) == 80\n",
    "    return EfficientNet(qm.QuantActivConv2d, version, archws, archas, num_classes=100, **kwargs)\n",
    "\n",
    "def quanteffnet_w3a3(arch_cfg_path, **kwargs):\n",
    "    version = \"b0\"\n",
    "    archas = [3] *80\n",
    "    archws = [3] *80\n",
    "    assert len(archas) == 80\n",
    "    assert len(archws) == 80\n",
    "    return EfficientNet(qm.QuantActivConv2d, version, archws, archas, num_classes=100, **kwargs)\n",
    "\n",
    "def quanteffnet_w2a2(arch_cfg_path, **kwargs):\n",
    "    version = \"b0\"\n",
    "    archas = [2] *80\n",
    "    archws = [2] *80\n",
    "    assert len(archas) == 80\n",
    "    assert len(archws) == 80\n",
    "    return EfficientNet(qm.QuantActivConv2d, version, archws, archas, num_classes=100, **kwargs)\n",
    "\n",
    "\n",
    "def quanteffnet_w8a8_chan(arch_cfg_path, **kwargs):\n",
    "    version = \"b0\"\n",
    "    archas = [8] *80\n",
    "    archws = [8] *80\n",
    "    assert len(archas) == 80\n",
    "    assert len(archws) == 80\n",
    "    return EfficientNet(qm.QuantMixActivChanConv2d, version, archws, archas, num_classes=100, **kwargs)\n",
    "\n",
    "def quanteffnet_cfg(arch_cfg_path, **kwargs):\n",
    "    wbits, abits = [1, 2, 3, 4], [2, 3, 4]\n",
    "    version = \"b0\"\n",
    "    name_nbits = {'alpha_activ': len(abits), 'alpha_weight': len(wbits)}\n",
    "    best_arch, worst_arch = _load_arch(arch_cfg_path, name_nbits)\n",
    "    archas = [abits[a] for a in best_arch['alpha_activ']]\n",
    "    archws = [wbits[w] for w in best_arch['alpha_weight']]\n",
    "    assert len(archas) == 80\n",
    "    assert len(archws) == 80\n",
    "    return EfficientNet(qm.QuantActivConv2d, version, archws, archas, num_classes=100, **kwargs)\n",
    "\n",
    "\n",
    "def quanteffnet_cfg_2468(arch_cfg_path, **kwargs):\n",
    "    wbits, abits = [2, 4, 6, 8],  [2, 4, 6, 8]\n",
    "    version = \"b0\"\n",
    "    name_nbits = {'alpha_activ': len(abits), 'alpha_weight': len(wbits)}\n",
    "    best_arch, worst_arch = _load_arch(arch_cfg_path, name_nbits)\n",
    "    archas = [abits[a] for a in best_arch['alpha_activ']]\n",
    "    archws = [wbits[w] for w in best_arch['alpha_weight']]\n",
    "    assert len(archas) == 80\n",
    "    assert len(archws) == 80\n",
    "    return EfficientNet(qm.QuantActivConv2d, version, archws, archas, num_classes=100, **kwargs)\n",
    "\n",
    "\n",
    "def quanteffnet_cfg_2468_b3(arch_cfg_path, **kwargs):\n",
    "    wbits, abits = [2, 4, 6, 8],  [2, 4, 6, 8]\n",
    "    version = \"b3\"\n",
    "    name_nbits = {'alpha_activ': len(abits), 'alpha_weight': len(wbits)}\n",
    "    best_arch, worst_arch = _load_arch(arch_cfg_path, name_nbits)\n",
    "    archas = [abits[a] for a in best_arch['alpha_activ']]\n",
    "    archws = [wbits[w] for w in best_arch['alpha_weight']]\n",
    "    # assert len(archas) == 80\n",
    "    # assert len(archws) == 80\n",
    "    return EfficientNet(qm.QuantActivConv2d, version, archws, archas, num_classes=100, **kwargs)\n",
    "\n",
    " \n",
    "def quanteffnet_w8a8_b3(arch_cfg_path, **kwargs):\n",
    "    version = \"b3\"\n",
    "    archas = [8] *129\n",
    "    archws = [8] *129\n",
    "    assert len(archas) == 129\n",
    "    assert len(archws) == 129\n",
    "    return EfficientNet(qm.QuantActivConv2d, version, archws, archas, num_classes=100, **kwargs)\n",
    "\n",
    " \n",
    "def quanteffnet_w32a32_chan(arch_cfg_path, **kwargs):\n",
    "    version = \"b0\"\n",
    "    archas = [32] *80\n",
    "    archws = [32] *80\n",
    "    assert len(archas) == 80\n",
    "    assert len(archws) == 80\n",
    "    return EfficientNet(qm.QuantMixActivChanConv2d, version, archws, archas, num_classes=100, **kwargs)\n",
    "\n",
    "def quanteffnet_w2a2_chan(arch_cfg_path, **kwargs):\n",
    "    version = \"b0\"\n",
    "    archas = [2] *80\n",
    "    archws = [2] *80\n",
    "    assert len(archas) == 80\n",
    "    assert len(archws) == 80\n",
    "    return EfficientNet(qm.QuantMixActivChanConv2d, version, archws, archas, num_classes=100, **kwargs)\n",
    "\n",
    "def quanteffnet_w4a4_chan(arch_cfg_path, **kwargs):\n",
    "    version = \"b0\"\n",
    "    archas = [4] *80\n",
    "    archws = [4] *80\n",
    "    assert len(archas) == 80\n",
    "    assert len(archws) == 80\n",
    "    return EfficientNet(qm.QuantMixActivChanConv2d, version, archws, archas, num_classes=100, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archas: [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "archws: [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (features): Sequential(\n",
       "    (0): BasicCNNBlock(\n",
       "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (1): InvertedResidualBlock(\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): QuantActivConv2d(\n",
       "            (activ): HWGQ()\n",
       "            (conv): QuantConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          )\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(16, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): QuantActivConv2d(\n",
       "            (activ): HWGQ()\n",
       "            (conv): QuantConv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          )\n",
       "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(96, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(4, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(24, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): QuantActivConv2d(\n",
       "            (activ): HWGQ()\n",
       "            (conv): QuantConv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          )\n",
       "          (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(144, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(6, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(24, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): QuantActivConv2d(\n",
       "            (activ): HWGQ()\n",
       "            (conv): QuantConv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "          )\n",
       "          (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(144, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(6, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(40, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): QuantActivConv2d(\n",
       "            (activ): HWGQ()\n",
       "            (conv): QuantConv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          )\n",
       "          (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(240, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(10, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(40, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): QuantActivConv2d(\n",
       "            (activ): HWGQ()\n",
       "            (conv): QuantConv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "          )\n",
       "          (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(240, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(10, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(80, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): QuantActivConv2d(\n",
       "            (activ): HWGQ()\n",
       "            (conv): QuantConv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          )\n",
       "          (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(80, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): QuantActivConv2d(\n",
       "            (activ): HWGQ()\n",
       "            (conv): QuantConv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          )\n",
       "          (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(80, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): QuantActivConv2d(\n",
       "            (activ): HWGQ()\n",
       "            (conv): QuantConv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "          )\n",
       "          (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(112, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): QuantActivConv2d(\n",
       "            (activ): HWGQ()\n",
       "            (conv): QuantConv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "          )\n",
       "          (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(112, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): QuantActivConv2d(\n",
       "            (activ): HWGQ()\n",
       "            (conv): QuantConv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "          )\n",
       "          (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(112, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): QuantActivConv2d(\n",
       "            (activ): HWGQ()\n",
       "            (conv): QuantConv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "          )\n",
       "          (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(192, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): QuantActivConv2d(\n",
       "            (activ): HWGQ()\n",
       "            (conv): QuantConv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          )\n",
       "          (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(192, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): QuantActivConv2d(\n",
       "            (activ): HWGQ()\n",
       "            (conv): QuantConv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          )\n",
       "          (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(192, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): QuantActivConv2d(\n",
       "            (activ): HWGQ()\n",
       "            (conv): QuantConv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          )\n",
       "          (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(192, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "        (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): QuantActivConv2d(\n",
       "            (activ): HWGQ()\n",
       "            (conv): QuantConv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "          )\n",
       "          (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): QuantActivConv2d(\n",
       "              (activ): HWGQ()\n",
       "              (conv): QuantConv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): QuantActivConv2d(\n",
       "          (activ): HWGQ()\n",
       "          (conv): QuantConv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): CNNBlock(\n",
       "      (cnn): QuantActivConv2d(\n",
       "        (activ): HWGQ()\n",
       "        (conv): QuantConv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "      (bn): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=100, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = quanteffnet_w8a8(\"a\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archas: [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "archws: [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "features.0: 0.0003879070281982422 seconds\n",
      "features.1: 0.17446517944335938 seconds\n",
      "features.2: 0.6893327236175537 seconds\n",
      "features.3: 0.5637657642364502 seconds\n",
      "features.4: 0.31708192825317383 seconds\n",
      "features.5: 0.12467694282531738 seconds\n",
      "features.6: 0.13201284408569336 seconds\n",
      "features.7: 0.022197723388671875 seconds\n",
      "features.8: 0.038918495178222656 seconds\n",
      "features.9: 0.027732372283935547 seconds\n",
      "features.10: 0.15244102478027344 seconds\n",
      "features.11: 0.05519533157348633 seconds\n",
      "features.12: 0.06907415390014648 seconds\n",
      "features.13: 0.06613039970397949 seconds\n",
      "features.14: 0.039960384368896484 seconds\n",
      "features.15: 0.06242680549621582 seconds\n",
      "features.16: 0.029285669326782227 seconds\n",
      "features.17: 0.0027790069580078125 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "class LayerTimer:\n",
    "    def __init__(self):\n",
    "        self.start_times = {}\n",
    "        self.durations = {}\n",
    "\n",
    "    def start(self, layer_name):\n",
    "        torch.cuda.synchronize()\n",
    "        self.start_times[layer_name] = time.time()\n",
    "\n",
    "    def stop(self, layer_name):\n",
    "        torch.cuda.synchronize()\n",
    "        duration = time.time() - self.start_times[layer_name]\n",
    "        if layer_name in self.durations:\n",
    "            self.durations[layer_name].append(duration)\n",
    "        else:\n",
    "            self.durations[layer_name] = [duration]\n",
    "\n",
    "def add_timing_hooks(layer, layer_timer, layer_name=\"\"):\n",
    "    if isinstance(layer, (InvertedResidualBlock, BasicCNNBlock, CNNBlock)):\n",
    "        # 対象のレイヤータイプにフックを追加\n",
    "        layer.register_forward_pre_hook(lambda layer, input: layer_timer.start(layer_name))\n",
    "        layer.register_forward_hook(lambda layer, input, output: layer_timer.stop(layer_name))\n",
    "    elif hasattr(layer, 'children') and len(list(layer.children())) > 0:\n",
    "        # 子モジュールがある場合、それぞれに対して再帰的にフックを追加\n",
    "        for name, child in layer.named_children():\n",
    "            add_timing_hooks(child, layer_timer, f\"{layer_name}.{name}\" if layer_name else name)\n",
    "\n",
    "\n",
    "model = quanteffnet_w8a8(\"a\").to('cuda:1')  # GPUを使用する場合\n",
    "model.eval()\n",
    "\n",
    "# レイヤータイマーの初期化\n",
    "layer_timer = LayerTimer()\n",
    "# フックの追加\n",
    "add_timing_hooks(model, layer_timer)\n",
    "\n",
    "# ダミーの入力データ\n",
    "input_data = torch.randn(30, 3, 224, 224).to('cuda:1')  # GPUを使用する場合\n",
    "# 推論の実行\n",
    "model(input_data)\n",
    "\n",
    "# 各InvertedResidualBlockの推論時間を表示\n",
    "for layer_name, durations in layer_timer.durations.items():\n",
    "    print(f\"{layer_name}: {sum(durations) / len(durations)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pool\n",
      "features\n",
      "features.0\n",
      "features.0.conv\n",
      "features.0.bn\n",
      "features.0.silu\n",
      "features.1\n",
      "features.1.conv\n",
      "features.1.conv.0\n",
      "features.1.conv.0.cnn\n",
      "features.1.conv.0.cnn.activ\n",
      "features.1.conv.0.cnn.conv\n",
      "features.1.conv.0.bn\n",
      "features.1.conv.0.silu\n",
      "features.1.conv.1\n",
      "features.1.conv.1.se\n",
      "features.1.conv.1.se.0\n",
      "features.1.conv.1.se.1\n",
      "features.1.conv.1.se.1.activ\n",
      "features.1.conv.1.se.1.conv\n",
      "features.1.conv.1.se.2\n",
      "features.1.conv.1.se.3\n",
      "features.1.conv.1.se.3.activ\n",
      "features.1.conv.1.se.3.conv\n",
      "features.1.conv.1.se.4\n",
      "features.1.conv.2\n",
      "features.1.conv.2.activ\n",
      "features.1.conv.2.conv\n",
      "features.1.conv.3\n",
      "features.2\n",
      "features.2.expand_conv\n",
      "features.2.expand_conv.cnn\n",
      "features.2.expand_conv.cnn.activ\n",
      "features.2.expand_conv.cnn.conv\n",
      "features.2.expand_conv.bn\n",
      "features.2.expand_conv.silu\n",
      "features.2.conv\n",
      "features.2.conv.0\n",
      "features.2.conv.0.cnn\n",
      "features.2.conv.0.cnn.activ\n",
      "features.2.conv.0.cnn.conv\n",
      "features.2.conv.0.bn\n",
      "features.2.conv.0.silu\n",
      "features.2.conv.1\n",
      "features.2.conv.1.se\n",
      "features.2.conv.1.se.0\n",
      "features.2.conv.1.se.1\n",
      "features.2.conv.1.se.1.activ\n",
      "features.2.conv.1.se.1.conv\n",
      "features.2.conv.1.se.2\n",
      "features.2.conv.1.se.3\n",
      "features.2.conv.1.se.3.activ\n",
      "features.2.conv.1.se.3.conv\n",
      "features.2.conv.1.se.4\n",
      "features.2.conv.2\n",
      "features.2.conv.2.activ\n",
      "features.2.conv.2.conv\n",
      "features.2.conv.3\n",
      "features.3\n",
      "features.3.expand_conv\n",
      "features.3.expand_conv.cnn\n",
      "features.3.expand_conv.cnn.activ\n",
      "features.3.expand_conv.cnn.conv\n",
      "features.3.expand_conv.bn\n",
      "features.3.expand_conv.silu\n",
      "features.3.conv\n",
      "features.3.conv.0\n",
      "features.3.conv.0.cnn\n",
      "features.3.conv.0.cnn.activ\n",
      "features.3.conv.0.cnn.conv\n",
      "features.3.conv.0.bn\n",
      "features.3.conv.0.silu\n",
      "features.3.conv.1\n",
      "features.3.conv.1.se\n",
      "features.3.conv.1.se.0\n",
      "features.3.conv.1.se.1\n",
      "features.3.conv.1.se.1.activ\n",
      "features.3.conv.1.se.1.conv\n",
      "features.3.conv.1.se.2\n",
      "features.3.conv.1.se.3\n",
      "features.3.conv.1.se.3.activ\n",
      "features.3.conv.1.se.3.conv\n",
      "features.3.conv.1.se.4\n",
      "features.3.conv.2\n",
      "features.3.conv.2.activ\n",
      "features.3.conv.2.conv\n",
      "features.3.conv.3\n",
      "features.4\n",
      "features.4.expand_conv\n",
      "features.4.expand_conv.cnn\n",
      "features.4.expand_conv.cnn.activ\n",
      "features.4.expand_conv.cnn.conv\n",
      "features.4.expand_conv.bn\n",
      "features.4.expand_conv.silu\n",
      "features.4.conv\n",
      "features.4.conv.0\n",
      "features.4.conv.0.cnn\n",
      "features.4.conv.0.cnn.activ\n",
      "features.4.conv.0.cnn.conv\n",
      "features.4.conv.0.bn\n",
      "features.4.conv.0.silu\n",
      "features.4.conv.1\n",
      "features.4.conv.1.se\n",
      "features.4.conv.1.se.0\n",
      "features.4.conv.1.se.1\n",
      "features.4.conv.1.se.1.activ\n",
      "features.4.conv.1.se.1.conv\n",
      "features.4.conv.1.se.2\n",
      "features.4.conv.1.se.3\n",
      "features.4.conv.1.se.3.activ\n",
      "features.4.conv.1.se.3.conv\n",
      "features.4.conv.1.se.4\n",
      "features.4.conv.2\n",
      "features.4.conv.2.activ\n",
      "features.4.conv.2.conv\n",
      "features.4.conv.3\n",
      "features.5\n",
      "features.5.expand_conv\n",
      "features.5.expand_conv.cnn\n",
      "features.5.expand_conv.cnn.activ\n",
      "features.5.expand_conv.cnn.conv\n",
      "features.5.expand_conv.bn\n",
      "features.5.expand_conv.silu\n",
      "features.5.conv\n",
      "features.5.conv.0\n",
      "features.5.conv.0.cnn\n",
      "features.5.conv.0.cnn.activ\n",
      "features.5.conv.0.cnn.conv\n",
      "features.5.conv.0.bn\n",
      "features.5.conv.0.silu\n",
      "features.5.conv.1\n",
      "features.5.conv.1.se\n",
      "features.5.conv.1.se.0\n",
      "features.5.conv.1.se.1\n",
      "features.5.conv.1.se.1.activ\n",
      "features.5.conv.1.se.1.conv\n",
      "features.5.conv.1.se.2\n",
      "features.5.conv.1.se.3\n",
      "features.5.conv.1.se.3.activ\n",
      "features.5.conv.1.se.3.conv\n",
      "features.5.conv.1.se.4\n",
      "features.5.conv.2\n",
      "features.5.conv.2.activ\n",
      "features.5.conv.2.conv\n",
      "features.5.conv.3\n",
      "features.6\n",
      "features.6.expand_conv\n",
      "features.6.expand_conv.cnn\n",
      "features.6.expand_conv.cnn.activ\n",
      "features.6.expand_conv.cnn.conv\n",
      "features.6.expand_conv.bn\n",
      "features.6.expand_conv.silu\n",
      "features.6.conv\n",
      "features.6.conv.0\n",
      "features.6.conv.0.cnn\n",
      "features.6.conv.0.cnn.activ\n",
      "features.6.conv.0.cnn.conv\n",
      "features.6.conv.0.bn\n",
      "features.6.conv.0.silu\n",
      "features.6.conv.1\n",
      "features.6.conv.1.se\n",
      "features.6.conv.1.se.0\n",
      "features.6.conv.1.se.1\n",
      "features.6.conv.1.se.1.activ\n",
      "features.6.conv.1.se.1.conv\n",
      "features.6.conv.1.se.2\n",
      "features.6.conv.1.se.3\n",
      "features.6.conv.1.se.3.activ\n",
      "features.6.conv.1.se.3.conv\n",
      "features.6.conv.1.se.4\n",
      "features.6.conv.2\n",
      "features.6.conv.2.activ\n",
      "features.6.conv.2.conv\n",
      "features.6.conv.3\n",
      "features.7\n",
      "features.7.expand_conv\n",
      "features.7.expand_conv.cnn\n",
      "features.7.expand_conv.cnn.activ\n",
      "features.7.expand_conv.cnn.conv\n",
      "features.7.expand_conv.bn\n",
      "features.7.expand_conv.silu\n",
      "features.7.conv\n",
      "features.7.conv.0\n",
      "features.7.conv.0.cnn\n",
      "features.7.conv.0.cnn.activ\n",
      "features.7.conv.0.cnn.conv\n",
      "features.7.conv.0.bn\n",
      "features.7.conv.0.silu\n",
      "features.7.conv.1\n",
      "features.7.conv.1.se\n",
      "features.7.conv.1.se.0\n",
      "features.7.conv.1.se.1\n",
      "features.7.conv.1.se.1.activ\n",
      "features.7.conv.1.se.1.conv\n",
      "features.7.conv.1.se.2\n",
      "features.7.conv.1.se.3\n",
      "features.7.conv.1.se.3.activ\n",
      "features.7.conv.1.se.3.conv\n",
      "features.7.conv.1.se.4\n",
      "features.7.conv.2\n",
      "features.7.conv.2.activ\n",
      "features.7.conv.2.conv\n",
      "features.7.conv.3\n",
      "features.8\n",
      "features.8.expand_conv\n",
      "features.8.expand_conv.cnn\n",
      "features.8.expand_conv.cnn.activ\n",
      "features.8.expand_conv.cnn.conv\n",
      "features.8.expand_conv.bn\n",
      "features.8.expand_conv.silu\n",
      "features.8.conv\n",
      "features.8.conv.0\n",
      "features.8.conv.0.cnn\n",
      "features.8.conv.0.cnn.activ\n",
      "features.8.conv.0.cnn.conv\n",
      "features.8.conv.0.bn\n",
      "features.8.conv.0.silu\n",
      "features.8.conv.1\n",
      "features.8.conv.1.se\n",
      "features.8.conv.1.se.0\n",
      "features.8.conv.1.se.1\n",
      "features.8.conv.1.se.1.activ\n",
      "features.8.conv.1.se.1.conv\n",
      "features.8.conv.1.se.2\n",
      "features.8.conv.1.se.3\n",
      "features.8.conv.1.se.3.activ\n",
      "features.8.conv.1.se.3.conv\n",
      "features.8.conv.1.se.4\n",
      "features.8.conv.2\n",
      "features.8.conv.2.activ\n",
      "features.8.conv.2.conv\n",
      "features.8.conv.3\n",
      "features.9\n",
      "features.9.expand_conv\n",
      "features.9.expand_conv.cnn\n",
      "features.9.expand_conv.cnn.activ\n",
      "features.9.expand_conv.cnn.conv\n",
      "features.9.expand_conv.bn\n",
      "features.9.expand_conv.silu\n",
      "features.9.conv\n",
      "features.9.conv.0\n",
      "features.9.conv.0.cnn\n",
      "features.9.conv.0.cnn.activ\n",
      "features.9.conv.0.cnn.conv\n",
      "features.9.conv.0.bn\n",
      "features.9.conv.0.silu\n",
      "features.9.conv.1\n",
      "features.9.conv.1.se\n",
      "features.9.conv.1.se.0\n",
      "features.9.conv.1.se.1\n",
      "features.9.conv.1.se.1.activ\n",
      "features.9.conv.1.se.1.conv\n",
      "features.9.conv.1.se.2\n",
      "features.9.conv.1.se.3\n",
      "features.9.conv.1.se.3.activ\n",
      "features.9.conv.1.se.3.conv\n",
      "features.9.conv.1.se.4\n",
      "features.9.conv.2\n",
      "features.9.conv.2.activ\n",
      "features.9.conv.2.conv\n",
      "features.9.conv.3\n",
      "features.10\n",
      "features.10.expand_conv\n",
      "features.10.expand_conv.cnn\n",
      "features.10.expand_conv.cnn.activ\n",
      "features.10.expand_conv.cnn.conv\n",
      "features.10.expand_conv.bn\n",
      "features.10.expand_conv.silu\n",
      "features.10.conv\n",
      "features.10.conv.0\n",
      "features.10.conv.0.cnn\n",
      "features.10.conv.0.cnn.activ\n",
      "features.10.conv.0.cnn.conv\n",
      "features.10.conv.0.bn\n",
      "features.10.conv.0.silu\n",
      "features.10.conv.1\n",
      "features.10.conv.1.se\n",
      "features.10.conv.1.se.0\n",
      "features.10.conv.1.se.1\n",
      "features.10.conv.1.se.1.activ\n",
      "features.10.conv.1.se.1.conv\n",
      "features.10.conv.1.se.2\n",
      "features.10.conv.1.se.3\n",
      "features.10.conv.1.se.3.activ\n",
      "features.10.conv.1.se.3.conv\n",
      "features.10.conv.1.se.4\n",
      "features.10.conv.2\n",
      "features.10.conv.2.activ\n",
      "features.10.conv.2.conv\n",
      "features.10.conv.3\n",
      "features.11\n",
      "features.11.expand_conv\n",
      "features.11.expand_conv.cnn\n",
      "features.11.expand_conv.cnn.activ\n",
      "features.11.expand_conv.cnn.conv\n",
      "features.11.expand_conv.bn\n",
      "features.11.expand_conv.silu\n",
      "features.11.conv\n",
      "features.11.conv.0\n",
      "features.11.conv.0.cnn\n",
      "features.11.conv.0.cnn.activ\n",
      "features.11.conv.0.cnn.conv\n",
      "features.11.conv.0.bn\n",
      "features.11.conv.0.silu\n",
      "features.11.conv.1\n",
      "features.11.conv.1.se\n",
      "features.11.conv.1.se.0\n",
      "features.11.conv.1.se.1\n",
      "features.11.conv.1.se.1.activ\n",
      "features.11.conv.1.se.1.conv\n",
      "features.11.conv.1.se.2\n",
      "features.11.conv.1.se.3\n",
      "features.11.conv.1.se.3.activ\n",
      "features.11.conv.1.se.3.conv\n",
      "features.11.conv.1.se.4\n",
      "features.11.conv.2\n",
      "features.11.conv.2.activ\n",
      "features.11.conv.2.conv\n",
      "features.11.conv.3\n",
      "features.12\n",
      "features.12.expand_conv\n",
      "features.12.expand_conv.cnn\n",
      "features.12.expand_conv.cnn.activ\n",
      "features.12.expand_conv.cnn.conv\n",
      "features.12.expand_conv.bn\n",
      "features.12.expand_conv.silu\n",
      "features.12.conv\n",
      "features.12.conv.0\n",
      "features.12.conv.0.cnn\n",
      "features.12.conv.0.cnn.activ\n",
      "features.12.conv.0.cnn.conv\n",
      "features.12.conv.0.bn\n",
      "features.12.conv.0.silu\n",
      "features.12.conv.1\n",
      "features.12.conv.1.se\n",
      "features.12.conv.1.se.0\n",
      "features.12.conv.1.se.1\n",
      "features.12.conv.1.se.1.activ\n",
      "features.12.conv.1.se.1.conv\n",
      "features.12.conv.1.se.2\n",
      "features.12.conv.1.se.3\n",
      "features.12.conv.1.se.3.activ\n",
      "features.12.conv.1.se.3.conv\n",
      "features.12.conv.1.se.4\n",
      "features.12.conv.2\n",
      "features.12.conv.2.activ\n",
      "features.12.conv.2.conv\n",
      "features.12.conv.3\n",
      "features.13\n",
      "features.13.expand_conv\n",
      "features.13.expand_conv.cnn\n",
      "features.13.expand_conv.cnn.activ\n",
      "features.13.expand_conv.cnn.conv\n",
      "features.13.expand_conv.bn\n",
      "features.13.expand_conv.silu\n",
      "features.13.conv\n",
      "features.13.conv.0\n",
      "features.13.conv.0.cnn\n",
      "features.13.conv.0.cnn.activ\n",
      "features.13.conv.0.cnn.conv\n",
      "features.13.conv.0.bn\n",
      "features.13.conv.0.silu\n",
      "features.13.conv.1\n",
      "features.13.conv.1.se\n",
      "features.13.conv.1.se.0\n",
      "features.13.conv.1.se.1\n",
      "features.13.conv.1.se.1.activ\n",
      "features.13.conv.1.se.1.conv\n",
      "features.13.conv.1.se.2\n",
      "features.13.conv.1.se.3\n",
      "features.13.conv.1.se.3.activ\n",
      "features.13.conv.1.se.3.conv\n",
      "features.13.conv.1.se.4\n",
      "features.13.conv.2\n",
      "features.13.conv.2.activ\n",
      "features.13.conv.2.conv\n",
      "features.13.conv.3\n",
      "features.14\n",
      "features.14.expand_conv\n",
      "features.14.expand_conv.cnn\n",
      "features.14.expand_conv.cnn.activ\n",
      "features.14.expand_conv.cnn.conv\n",
      "features.14.expand_conv.bn\n",
      "features.14.expand_conv.silu\n",
      "features.14.conv\n",
      "features.14.conv.0\n",
      "features.14.conv.0.cnn\n",
      "features.14.conv.0.cnn.activ\n",
      "features.14.conv.0.cnn.conv\n",
      "features.14.conv.0.bn\n",
      "features.14.conv.0.silu\n",
      "features.14.conv.1\n",
      "features.14.conv.1.se\n",
      "features.14.conv.1.se.0\n",
      "features.14.conv.1.se.1\n",
      "features.14.conv.1.se.1.activ\n",
      "features.14.conv.1.se.1.conv\n",
      "features.14.conv.1.se.2\n",
      "features.14.conv.1.se.3\n",
      "features.14.conv.1.se.3.activ\n",
      "features.14.conv.1.se.3.conv\n",
      "features.14.conv.1.se.4\n",
      "features.14.conv.2\n",
      "features.14.conv.2.activ\n",
      "features.14.conv.2.conv\n",
      "features.14.conv.3\n",
      "features.15\n",
      "features.15.expand_conv\n",
      "features.15.expand_conv.cnn\n",
      "features.15.expand_conv.cnn.activ\n",
      "features.15.expand_conv.cnn.conv\n",
      "features.15.expand_conv.bn\n",
      "features.15.expand_conv.silu\n",
      "features.15.conv\n",
      "features.15.conv.0\n",
      "features.15.conv.0.cnn\n",
      "features.15.conv.0.cnn.activ\n",
      "features.15.conv.0.cnn.conv\n",
      "features.15.conv.0.bn\n",
      "features.15.conv.0.silu\n",
      "features.15.conv.1\n",
      "features.15.conv.1.se\n",
      "features.15.conv.1.se.0\n",
      "features.15.conv.1.se.1\n",
      "features.15.conv.1.se.1.activ\n",
      "features.15.conv.1.se.1.conv\n",
      "features.15.conv.1.se.2\n",
      "features.15.conv.1.se.3\n",
      "features.15.conv.1.se.3.activ\n",
      "features.15.conv.1.se.3.conv\n",
      "features.15.conv.1.se.4\n",
      "features.15.conv.2\n",
      "features.15.conv.2.activ\n",
      "features.15.conv.2.conv\n",
      "features.15.conv.3\n",
      "features.16\n",
      "features.16.expand_conv\n",
      "features.16.expand_conv.cnn\n",
      "features.16.expand_conv.cnn.activ\n",
      "features.16.expand_conv.cnn.conv\n",
      "features.16.expand_conv.bn\n",
      "features.16.expand_conv.silu\n",
      "features.16.conv\n",
      "features.16.conv.0\n",
      "features.16.conv.0.cnn\n",
      "features.16.conv.0.cnn.activ\n",
      "features.16.conv.0.cnn.conv\n",
      "features.16.conv.0.bn\n",
      "features.16.conv.0.silu\n",
      "features.16.conv.1\n",
      "features.16.conv.1.se\n",
      "features.16.conv.1.se.0\n",
      "features.16.conv.1.se.1\n",
      "features.16.conv.1.se.1.activ\n",
      "features.16.conv.1.se.1.conv\n",
      "features.16.conv.1.se.2\n",
      "features.16.conv.1.se.3\n",
      "features.16.conv.1.se.3.activ\n",
      "features.16.conv.1.se.3.conv\n",
      "features.16.conv.1.se.4\n",
      "features.16.conv.2\n",
      "features.16.conv.2.activ\n",
      "features.16.conv.2.conv\n",
      "features.16.conv.3\n",
      "features.17\n",
      "features.17.cnn\n",
      "features.17.cnn.activ\n",
      "features.17.cnn.conv\n",
      "features.17.bn\n",
      "features.17.silu\n",
      "classifier\n",
      "classifier.0\n",
      "classifier.1\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Hswish()\n",
      "  )\n",
      "  (1): MobileBottleneck(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
      "      (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): SEModule(\n",
      "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=16, out_features=4, bias=False)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Linear(in_features=4, out_features=16, bias=False)\n",
      "          (3): Hsigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (2): MobileBottleneck(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "      (4): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Identity()\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (8): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (3): MobileBottleneck(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
      "      (4): BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): Identity()\n",
      "      (6): ReLU(inplace=True)\n",
      "      (7): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (8): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (4): MobileBottleneck(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Hswish()\n",
      "      (3): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
      "      (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): SEModule(\n",
      "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=96, out_features=24, bias=False)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Linear(in_features=24, out_features=96, bias=False)\n",
      "          (3): Hsigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): Hswish()\n",
      "      (7): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (8): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (5): MobileBottleneck(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Hswish()\n",
      "      (3): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "      (4): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): SEModule(\n",
      "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=240, out_features=60, bias=False)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Linear(in_features=60, out_features=240, bias=False)\n",
      "          (3): Hsigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): Hswish()\n",
      "      (7): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (8): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (6): MobileBottleneck(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Hswish()\n",
      "      (3): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "      (4): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): SEModule(\n",
      "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=240, out_features=60, bias=False)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Linear(in_features=60, out_features=240, bias=False)\n",
      "          (3): Hsigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): Hswish()\n",
      "      (7): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (8): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (7): MobileBottleneck(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Hswish()\n",
      "      (3): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "      (4): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): SEModule(\n",
      "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=120, out_features=30, bias=False)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Linear(in_features=30, out_features=120, bias=False)\n",
      "          (3): Hsigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): Hswish()\n",
      "      (7): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (8): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (8): MobileBottleneck(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Hswish()\n",
      "      (3): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "      (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): SEModule(\n",
      "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=144, out_features=36, bias=False)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Linear(in_features=36, out_features=144, bias=False)\n",
      "          (3): Hsigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): Hswish()\n",
      "      (7): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (8): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (9): MobileBottleneck(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Hswish()\n",
      "      (3): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
      "      (4): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): SEModule(\n",
      "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=288, out_features=72, bias=False)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Linear(in_features=72, out_features=288, bias=False)\n",
      "          (3): Hsigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): Hswish()\n",
      "      (7): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (10): MobileBottleneck(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Hswish()\n",
      "      (3): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "      (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): SEModule(\n",
      "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=576, out_features=144, bias=False)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Linear(in_features=144, out_features=576, bias=False)\n",
      "          (3): Hsigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): Hswish()\n",
      "      (7): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (11): MobileBottleneck(\n",
      "    (conv): Sequential(\n",
      "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Hswish()\n",
      "      (3): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "      (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): SEModule(\n",
      "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=576, out_features=144, bias=False)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Linear(in_features=144, out_features=576, bias=False)\n",
      "          (3): Hsigmoid()\n",
      "        )\n",
      "      )\n",
      "      (6): Hswish()\n",
      "      (7): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (8): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (12): Sequential(\n",
      "    (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Hswish()\n",
      "  )\n",
      "  (13): AdaptiveAvgPool2d(output_size=1)\n",
      "  (14): Conv2d(576, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (15): Hswish()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# print(model.features)\n",
    "\n",
    "# for module in model.features[1].modules():\n",
    "    # print(module)\n",
    "# MixActivConv2dを探すみたいな\n",
    "\n",
    "print(model.features)\n",
    "# for module in model.modules():\n",
    "#     print(module)\n",
    "\n",
    "\n",
    "# for name, layer in model.named_children():\n",
    "#     print(name, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " model: <class '__main__.MobileNetV3'>\n",
      "  model.features: <class 'torch.nn.modules.container.Sequential'>\n",
      "   model.features[0]: <class 'torch.nn.modules.container.Sequential'>\n",
      "    model.features[0][0]: Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    model.features[0][1]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    model.features[0][2]: Hswish()\n",
      "   model.features[1]: <class '__main__.MobileBottleneck'>\n",
      "    model.features[1].conv: <class 'torch.nn.modules.container.Sequential'>\n",
      "     model.features[1].conv[0]: Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[1].conv[1]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[1].conv[2]: ReLU(inplace=True)\n",
      "     model.features[1].conv[3]: Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
      "     model.features[1].conv[4]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[1].conv[5]: <class '__main__.SEModule'>\n",
      "      model.features[1].conv[5].avg_pool: AdaptiveAvgPool2d(output_size=1)\n",
      "      model.features[1].conv[5].fc: <class 'torch.nn.modules.container.Sequential'>\n",
      "       model.features[1].conv[5].fc[0]: Linear(in_features=16, out_features=4, bias=False)\n",
      "       model.features[1].conv[5].fc[1]: ReLU(inplace=True)\n",
      "       model.features[1].conv[5].fc[2]: Linear(in_features=4, out_features=16, bias=False)\n",
      "       model.features[1].conv[5].fc[3]: Hsigmoid()\n",
      "     model.features[1].conv[6]: ReLU(inplace=True)\n",
      "     model.features[1].conv[7]: Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[1].conv[8]: BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "   model.features[2]: <class '__main__.MobileBottleneck'>\n",
      "    model.features[2].conv: <class 'torch.nn.modules.container.Sequential'>\n",
      "     model.features[2].conv[0]: Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[2].conv[1]: BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[2].conv[2]: ReLU(inplace=True)\n",
      "     model.features[2].conv[3]: Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
      "     model.features[2].conv[4]: BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[2].conv[5]: Identity()\n",
      "     model.features[2].conv[6]: ReLU(inplace=True)\n",
      "     model.features[2].conv[7]: Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[2].conv[8]: BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "   model.features[3]: <class '__main__.MobileBottleneck'>\n",
      "    model.features[3].conv: <class 'torch.nn.modules.container.Sequential'>\n",
      "     model.features[3].conv[0]: Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[3].conv[1]: BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[3].conv[2]: ReLU(inplace=True)\n",
      "     model.features[3].conv[3]: Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
      "     model.features[3].conv[4]: BatchNorm2d(88, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[3].conv[5]: Identity()\n",
      "     model.features[3].conv[6]: ReLU(inplace=True)\n",
      "     model.features[3].conv[7]: Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[3].conv[8]: BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "   model.features[4]: <class '__main__.MobileBottleneck'>\n",
      "    model.features[4].conv: <class 'torch.nn.modules.container.Sequential'>\n",
      "     model.features[4].conv[0]: Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[4].conv[1]: BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[4].conv[2]: Hswish()\n",
      "     model.features[4].conv[3]: Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
      "     model.features[4].conv[4]: BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[4].conv[5]: <class '__main__.SEModule'>\n",
      "      model.features[4].conv[5].avg_pool: AdaptiveAvgPool2d(output_size=1)\n",
      "      model.features[4].conv[5].fc: <class 'torch.nn.modules.container.Sequential'>\n",
      "       model.features[4].conv[5].fc[0]: Linear(in_features=96, out_features=24, bias=False)\n",
      "       model.features[4].conv[5].fc[1]: ReLU(inplace=True)\n",
      "       model.features[4].conv[5].fc[2]: Linear(in_features=24, out_features=96, bias=False)\n",
      "       model.features[4].conv[5].fc[3]: Hsigmoid()\n",
      "     model.features[4].conv[6]: Hswish()\n",
      "     model.features[4].conv[7]: Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[4].conv[8]: BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "   model.features[5]: <class '__main__.MobileBottleneck'>\n",
      "    model.features[5].conv: <class 'torch.nn.modules.container.Sequential'>\n",
      "     model.features[5].conv[0]: Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[5].conv[1]: BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[5].conv[2]: Hswish()\n",
      "     model.features[5].conv[3]: Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "     model.features[5].conv[4]: BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[5].conv[5]: <class '__main__.SEModule'>\n",
      "      model.features[5].conv[5].avg_pool: AdaptiveAvgPool2d(output_size=1)\n",
      "      model.features[5].conv[5].fc: <class 'torch.nn.modules.container.Sequential'>\n",
      "       model.features[5].conv[5].fc[0]: Linear(in_features=240, out_features=60, bias=False)\n",
      "       model.features[5].conv[5].fc[1]: ReLU(inplace=True)\n",
      "       model.features[5].conv[5].fc[2]: Linear(in_features=60, out_features=240, bias=False)\n",
      "       model.features[5].conv[5].fc[3]: Hsigmoid()\n",
      "     model.features[5].conv[6]: Hswish()\n",
      "     model.features[5].conv[7]: Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[5].conv[8]: BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "   model.features[6]: <class '__main__.MobileBottleneck'>\n",
      "    model.features[6].conv: <class 'torch.nn.modules.container.Sequential'>\n",
      "     model.features[6].conv[0]: Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[6].conv[1]: BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[6].conv[2]: Hswish()\n",
      "     model.features[6].conv[3]: Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
      "     model.features[6].conv[4]: BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[6].conv[5]: <class '__main__.SEModule'>\n",
      "      model.features[6].conv[5].avg_pool: AdaptiveAvgPool2d(output_size=1)\n",
      "      model.features[6].conv[5].fc: <class 'torch.nn.modules.container.Sequential'>\n",
      "       model.features[6].conv[5].fc[0]: Linear(in_features=240, out_features=60, bias=False)\n",
      "       model.features[6].conv[5].fc[1]: ReLU(inplace=True)\n",
      "       model.features[6].conv[5].fc[2]: Linear(in_features=60, out_features=240, bias=False)\n",
      "       model.features[6].conv[5].fc[3]: Hsigmoid()\n",
      "     model.features[6].conv[6]: Hswish()\n",
      "     model.features[6].conv[7]: Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[6].conv[8]: BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "   model.features[7]: <class '__main__.MobileBottleneck'>\n",
      "    model.features[7].conv: <class 'torch.nn.modules.container.Sequential'>\n",
      "     model.features[7].conv[0]: Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[7].conv[1]: BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[7].conv[2]: Hswish()\n",
      "     model.features[7].conv[3]: Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "     model.features[7].conv[4]: BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[7].conv[5]: <class '__main__.SEModule'>\n",
      "      model.features[7].conv[5].avg_pool: AdaptiveAvgPool2d(output_size=1)\n",
      "      model.features[7].conv[5].fc: <class 'torch.nn.modules.container.Sequential'>\n",
      "       model.features[7].conv[5].fc[0]: Linear(in_features=120, out_features=30, bias=False)\n",
      "       model.features[7].conv[5].fc[1]: ReLU(inplace=True)\n",
      "       model.features[7].conv[5].fc[2]: Linear(in_features=30, out_features=120, bias=False)\n",
      "       model.features[7].conv[5].fc[3]: Hsigmoid()\n",
      "     model.features[7].conv[6]: Hswish()\n",
      "     model.features[7].conv[7]: Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[7].conv[8]: BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "   model.features[8]: <class '__main__.MobileBottleneck'>\n",
      "    model.features[8].conv: <class 'torch.nn.modules.container.Sequential'>\n",
      "     model.features[8].conv[0]: Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[8].conv[1]: BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[8].conv[2]: Hswish()\n",
      "     model.features[8].conv[3]: Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
      "     model.features[8].conv[4]: BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[8].conv[5]: <class '__main__.SEModule'>\n",
      "      model.features[8].conv[5].avg_pool: AdaptiveAvgPool2d(output_size=1)\n",
      "      model.features[8].conv[5].fc: <class 'torch.nn.modules.container.Sequential'>\n",
      "       model.features[8].conv[5].fc[0]: Linear(in_features=144, out_features=36, bias=False)\n",
      "       model.features[8].conv[5].fc[1]: ReLU(inplace=True)\n",
      "       model.features[8].conv[5].fc[2]: Linear(in_features=36, out_features=144, bias=False)\n",
      "       model.features[8].conv[5].fc[3]: Hsigmoid()\n",
      "     model.features[8].conv[6]: Hswish()\n",
      "     model.features[8].conv[7]: Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[8].conv[8]: BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "   model.features[9]: <class '__main__.MobileBottleneck'>\n",
      "    model.features[9].conv: <class 'torch.nn.modules.container.Sequential'>\n",
      "     model.features[9].conv[0]: Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[9].conv[1]: BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[9].conv[2]: Hswish()\n",
      "     model.features[9].conv[3]: Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
      "     model.features[9].conv[4]: BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[9].conv[5]: <class '__main__.SEModule'>\n",
      "      model.features[9].conv[5].avg_pool: AdaptiveAvgPool2d(output_size=1)\n",
      "      model.features[9].conv[5].fc: <class 'torch.nn.modules.container.Sequential'>\n",
      "       model.features[9].conv[5].fc[0]: Linear(in_features=288, out_features=72, bias=False)\n",
      "       model.features[9].conv[5].fc[1]: ReLU(inplace=True)\n",
      "       model.features[9].conv[5].fc[2]: Linear(in_features=72, out_features=288, bias=False)\n",
      "       model.features[9].conv[5].fc[3]: Hsigmoid()\n",
      "     model.features[9].conv[6]: Hswish()\n",
      "     model.features[9].conv[7]: Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[9].conv[8]: BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "   model.features[10]: <class '__main__.MobileBottleneck'>\n",
      "    model.features[10].conv: <class 'torch.nn.modules.container.Sequential'>\n",
      "     model.features[10].conv[0]: Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[10].conv[1]: BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[10].conv[2]: Hswish()\n",
      "     model.features[10].conv[3]: Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "     model.features[10].conv[4]: BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[10].conv[5]: <class '__main__.SEModule'>\n",
      "      model.features[10].conv[5].avg_pool: AdaptiveAvgPool2d(output_size=1)\n",
      "      model.features[10].conv[5].fc: <class 'torch.nn.modules.container.Sequential'>\n",
      "       model.features[10].conv[5].fc[0]: Linear(in_features=576, out_features=144, bias=False)\n",
      "       model.features[10].conv[5].fc[1]: ReLU(inplace=True)\n",
      "       model.features[10].conv[5].fc[2]: Linear(in_features=144, out_features=576, bias=False)\n",
      "       model.features[10].conv[5].fc[3]: Hsigmoid()\n",
      "     model.features[10].conv[6]: Hswish()\n",
      "     model.features[10].conv[7]: Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[10].conv[8]: BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "   model.features[11]: <class '__main__.MobileBottleneck'>\n",
      "    model.features[11].conv: <class 'torch.nn.modules.container.Sequential'>\n",
      "     model.features[11].conv[0]: Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[11].conv[1]: BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[11].conv[2]: Hswish()\n",
      "     model.features[11].conv[3]: Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
      "     model.features[11].conv[4]: BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "     model.features[11].conv[5]: <class '__main__.SEModule'>\n",
      "      model.features[11].conv[5].avg_pool: AdaptiveAvgPool2d(output_size=1)\n",
      "      model.features[11].conv[5].fc: <class 'torch.nn.modules.container.Sequential'>\n",
      "       model.features[11].conv[5].fc[0]: Linear(in_features=576, out_features=144, bias=False)\n",
      "       model.features[11].conv[5].fc[1]: ReLU(inplace=True)\n",
      "       model.features[11].conv[5].fc[2]: Linear(in_features=144, out_features=576, bias=False)\n",
      "       model.features[11].conv[5].fc[3]: Hsigmoid()\n",
      "     model.features[11].conv[6]: Hswish()\n",
      "     model.features[11].conv[7]: Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "     model.features[11].conv[8]: BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "   model.features[12]: <class 'torch.nn.modules.container.Sequential'>\n",
      "    model.features[12][0]: Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    model.features[12][1]: BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    model.features[12][2]: Hswish()\n",
      "   model.features[13]: AdaptiveAvgPool2d(output_size=1)\n",
      "   model.features[14]: Conv2d(576, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
      "   model.features[15]: Hswish()\n",
      "  model.classifier: <class 'torch.nn.modules.container.Sequential'>\n",
      "   model.classifier[0]: Dropout(p=0.8, inplace=False)\n",
      "   model.classifier[1]: Linear(in_features=1280, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "def print_model(module, name=\"model\", depth=0):\n",
    "    if len(list(module.named_children())) == 0:\n",
    "        print(f\"{' ' * depth} {name}: {module}\")\n",
    "    else:\n",
    "        print(f\"{' ' * depth} {name}: {type(module)}\")\n",
    "\n",
    "    for child_name, child_module in module.named_children():\n",
    "        if isinstance(module, torch.nn.Sequential):\n",
    "            child_name = f\"{name}[{child_name}]\"\n",
    "        else:\n",
    "            child_name = f\"{name}.{child_name}\"\n",
    "        print_model(child_module, child_name, depth + 1)\n",
    "\n",
    "\n",
    "print_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "\n",
    "# モデル\n",
    "model = models.resnet18()\n",
    "# model = mixeffnet_b3_w2468a2468_100()\n",
    "# create_feature_extractorを使用する場合\n",
    "\n",
    "\n",
    "feature_extractor = create_feature_extractor(model, [\"layer1\"])\n",
    "x = torch.randn(1, 3, 224, 224)\n",
    "features = feature_extractor(x)\n",
    "\n",
    "print(features[\"layer1\"].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Conv2d layers: 4\n"
     ]
    }
   ],
   "source": [
    "def count_conv2d_layers(model):\n",
    "    count = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            count += 1\n",
    "        elif isinstance(module, nn.Sequential):\n",
    "            # Sequentialブロック内でさらにConv2dを探す\n",
    "            for sub_module in module:\n",
    "                if isinstance(sub_module, nn.Conv2d):\n",
    "                    count += 1\n",
    "    return count\n",
    "\n",
    "num_conv2d = count_conv2d_layers(model.features[1])\n",
    "print(f\"Total number of Conv2d layers: {num_conv2d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block: features.1, Number of CNNs: 4\n",
      "Block: features.2, Number of CNNs: 5\n",
      "Block: features.3, Number of CNNs: 5\n",
      "Block: features.4, Number of CNNs: 5\n",
      "Block: features.5, Number of CNNs: 5\n",
      "Block: features.6, Number of CNNs: 5\n",
      "Block: features.7, Number of CNNs: 5\n",
      "Block: features.8, Number of CNNs: 5\n",
      "Block: features.9, Number of CNNs: 5\n",
      "Block: features.10, Number of CNNs: 5\n",
      "Block: features.11, Number of CNNs: 5\n",
      "Block: features.12, Number of CNNs: 5\n",
      "Block: features.13, Number of CNNs: 5\n",
      "Block: features.14, Number of CNNs: 5\n",
      "Block: features.15, Number of CNNs: 5\n",
      "Block: features.16, Number of CNNs: 5\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, InvertedResidualBlock):\n",
    "        cnn_count = sum(1 for _ in module.modules() if isinstance(_, nn.Conv2d))\n",
    "        print(f\"Block: {name}, Number of CNNs: {cnn_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1104, -0.0256, -0.1149,  0.1797,  0.2848, -0.2497, -0.0322,  0.1326,\n",
      "          0.0137,  0.1957]], grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abits: [2, 3, 4]\n",
      "wbits: [1, 2, 3, 4]\n",
      "Block: features.1, Number of CNNs: 4\n",
      "Error processing block features.1: Given groups=32, weight of size [32, 1, 3, 3], expected input[1, 3, 224, 224] to have 32 channels, but got 3 channels instead\n"
     ]
    }
   ],
   "source": [
    "def test_inverted_residual_blocks(model, input_size):\n",
    "    device = next(model.parameters()).device\n",
    "    mock_input = torch.randn(1, 3, input_size, input_size).to(device)\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, InvertedResidualBlock):\n",
    "            cnn_count = sum(1 for _ in module.modules() if isinstance(_, nn.Conv2d))\n",
    "            print(f\"Block: {name}, Number of CNNs: {cnn_count}\")\n",
    "\n",
    "            try:\n",
    "                # モック入力をブロックに適用して入力サイズをテスト\n",
    "                output = module(mock_input)\n",
    "                print(f\"Block: {name}, Input size: {mock_input.shape}, Output size: {output.shape}\")\n",
    "                mock_input = output  # 次のブロックのために出力を入力として更新\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error processing block {name}: {e}\")\n",
    "                break\n",
    "\n",
    "# モデルのインスタンス化\n",
    "model = mixeffnet_b0_w1234a234()  # または他のバージョン\n",
    "\n",
    "# ブロックのテスト\n",
    "test_inverted_residual_blocks(model, input_size=224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module: features.0.conv, Input size: torch.Size([1, 3, 224, 224]), Output size: torch.Size([1, 38, 112, 112])\n",
      "Module: features.1.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 38, 112, 112]), Output size: torch.Size([1, 38, 112, 112])\n",
      "Module: features.1.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 38, 112, 112]), Output size: torch.Size([1, 9, 112, 112])\n",
      "Module: features.1.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 9, 112, 112]), Output size: torch.Size([1, 38, 112, 112])\n",
      "Module: features.1.conv.2.mix_weight.conv, Input size: torch.Size([1, 38, 112, 112]), Output size: torch.Size([1, 20, 112, 112])\n",
      "Module: features.2.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 20, 112, 112]), Output size: torch.Size([1, 20, 112, 112])\n",
      "Module: features.2.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 20, 112, 112]), Output size: torch.Size([1, 5, 112, 112])\n",
      "Module: features.2.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 5, 112, 112]), Output size: torch.Size([1, 20, 112, 112])\n",
      "Module: features.2.conv.2.mix_weight.conv, Input size: torch.Size([1, 20, 112, 112]), Output size: torch.Size([1, 20, 112, 112])\n",
      "Module: features.3.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 20, 112, 112]), Output size: torch.Size([1, 120, 112, 112])\n",
      "Module: features.3.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 120, 112, 112]), Output size: torch.Size([1, 120, 56, 56])\n",
      "Module: features.3.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 120, 56, 56]), Output size: torch.Size([1, 5, 56, 56])\n",
      "Module: features.3.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 5, 56, 56]), Output size: torch.Size([1, 120, 56, 56])\n",
      "Module: features.3.conv.2.mix_weight.conv, Input size: torch.Size([1, 120, 56, 56]), Output size: torch.Size([1, 32, 56, 56])\n",
      "Module: features.4.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 32, 56, 56]), Output size: torch.Size([1, 192, 56, 56])\n",
      "Module: features.4.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 192, 56, 56]), Output size: torch.Size([1, 192, 56, 56])\n",
      "Module: features.4.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 192, 56, 56]), Output size: torch.Size([1, 8, 56, 56])\n",
      "Module: features.4.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 8, 56, 56]), Output size: torch.Size([1, 192, 56, 56])\n",
      "Module: features.4.conv.2.mix_weight.conv, Input size: torch.Size([1, 192, 56, 56]), Output size: torch.Size([1, 32, 56, 56])\n",
      "Module: features.5.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 32, 56, 56]), Output size: torch.Size([1, 192, 56, 56])\n",
      "Module: features.5.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 192, 56, 56]), Output size: torch.Size([1, 192, 56, 56])\n",
      "Module: features.5.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 192, 56, 56]), Output size: torch.Size([1, 8, 56, 56])\n",
      "Module: features.5.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 8, 56, 56]), Output size: torch.Size([1, 192, 56, 56])\n",
      "Module: features.5.conv.2.mix_weight.conv, Input size: torch.Size([1, 192, 56, 56]), Output size: torch.Size([1, 32, 56, 56])\n",
      "Module: features.6.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 32, 56, 56]), Output size: torch.Size([1, 192, 56, 56])\n",
      "Module: features.6.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 192, 56, 56]), Output size: torch.Size([1, 192, 28, 28])\n",
      "Module: features.6.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 192, 28, 28]), Output size: torch.Size([1, 8, 28, 28])\n",
      "Module: features.6.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 8, 28, 28]), Output size: torch.Size([1, 192, 28, 28])\n",
      "Module: features.6.conv.2.mix_weight.conv, Input size: torch.Size([1, 192, 28, 28]), Output size: torch.Size([1, 48, 28, 28])\n",
      "Module: features.7.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 48, 28, 28]), Output size: torch.Size([1, 288, 28, 28])\n",
      "Module: features.7.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 288, 28, 28]), Output size: torch.Size([1, 288, 28, 28])\n",
      "Module: features.7.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 288, 28, 28]), Output size: torch.Size([1, 12, 28, 28])\n",
      "Module: features.7.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 12, 28, 28]), Output size: torch.Size([1, 288, 28, 28])\n",
      "Module: features.7.conv.2.mix_weight.conv, Input size: torch.Size([1, 288, 28, 28]), Output size: torch.Size([1, 48, 28, 28])\n",
      "Module: features.8.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 48, 28, 28]), Output size: torch.Size([1, 288, 28, 28])\n",
      "Module: features.8.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 288, 28, 28]), Output size: torch.Size([1, 288, 28, 28])\n",
      "Module: features.8.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 288, 28, 28]), Output size: torch.Size([1, 12, 28, 28])\n",
      "Module: features.8.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 12, 28, 28]), Output size: torch.Size([1, 288, 28, 28])\n",
      "Module: features.8.conv.2.mix_weight.conv, Input size: torch.Size([1, 288, 28, 28]), Output size: torch.Size([1, 48, 28, 28])\n",
      "Module: features.9.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 48, 28, 28]), Output size: torch.Size([1, 288, 28, 28])\n",
      "Module: features.9.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 288, 28, 28]), Output size: torch.Size([1, 288, 14, 14])\n",
      "Module: features.9.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 288, 14, 14]), Output size: torch.Size([1, 12, 14, 14])\n",
      "Module: features.9.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 12, 14, 14]), Output size: torch.Size([1, 288, 14, 14])\n",
      "Module: features.9.conv.2.mix_weight.conv, Input size: torch.Size([1, 288, 14, 14]), Output size: torch.Size([1, 96, 14, 14])\n",
      "Module: features.10.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 96, 14, 14]), Output size: torch.Size([1, 576, 14, 14])\n",
      "Module: features.10.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 576, 14, 14]), Output size: torch.Size([1, 576, 14, 14])\n",
      "Module: features.10.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 576, 14, 14]), Output size: torch.Size([1, 24, 14, 14])\n",
      "Module: features.10.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 24, 14, 14]), Output size: torch.Size([1, 576, 14, 14])\n",
      "Module: features.10.conv.2.mix_weight.conv, Input size: torch.Size([1, 576, 14, 14]), Output size: torch.Size([1, 96, 14, 14])\n",
      "Module: features.11.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 96, 14, 14]), Output size: torch.Size([1, 576, 14, 14])\n",
      "Module: features.11.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 576, 14, 14]), Output size: torch.Size([1, 576, 14, 14])\n",
      "Module: features.11.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 576, 14, 14]), Output size: torch.Size([1, 24, 14, 14])\n",
      "Module: features.11.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 24, 14, 14]), Output size: torch.Size([1, 576, 14, 14])\n",
      "Module: features.11.conv.2.mix_weight.conv, Input size: torch.Size([1, 576, 14, 14]), Output size: torch.Size([1, 96, 14, 14])\n",
      "Module: features.12.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 96, 14, 14]), Output size: torch.Size([1, 576, 14, 14])\n",
      "Module: features.12.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 576, 14, 14]), Output size: torch.Size([1, 576, 14, 14])\n",
      "Module: features.12.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 576, 14, 14]), Output size: torch.Size([1, 24, 14, 14])\n",
      "Module: features.12.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 24, 14, 14]), Output size: torch.Size([1, 576, 14, 14])\n",
      "Module: features.12.conv.2.mix_weight.conv, Input size: torch.Size([1, 576, 14, 14]), Output size: torch.Size([1, 96, 14, 14])\n",
      "Module: features.13.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 96, 14, 14]), Output size: torch.Size([1, 576, 14, 14])\n",
      "Module: features.13.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 576, 14, 14]), Output size: torch.Size([1, 576, 14, 14])\n",
      "Module: features.13.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 576, 14, 14]), Output size: torch.Size([1, 24, 14, 14])\n",
      "Module: features.13.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 24, 14, 14]), Output size: torch.Size([1, 576, 14, 14])\n",
      "Module: features.13.conv.2.mix_weight.conv, Input size: torch.Size([1, 576, 14, 14]), Output size: torch.Size([1, 96, 14, 14])\n",
      "Module: features.14.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 96, 14, 14]), Output size: torch.Size([1, 576, 14, 14])\n",
      "Module: features.14.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 576, 14, 14]), Output size: torch.Size([1, 576, 14, 14])\n",
      "Module: features.14.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 576, 14, 14]), Output size: torch.Size([1, 24, 14, 14])\n",
      "Module: features.14.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 24, 14, 14]), Output size: torch.Size([1, 576, 14, 14])\n",
      "Module: features.14.conv.2.mix_weight.conv, Input size: torch.Size([1, 576, 14, 14]), Output size: torch.Size([1, 136, 14, 14])\n",
      "Module: features.15.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 136, 14, 14]), Output size: torch.Size([1, 816, 14, 14])\n",
      "Module: features.15.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 816, 14, 14]), Output size: torch.Size([1, 816, 14, 14])\n",
      "Module: features.15.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 816, 14, 14]), Output size: torch.Size([1, 34, 14, 14])\n",
      "Module: features.15.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 34, 14, 14]), Output size: torch.Size([1, 816, 14, 14])\n",
      "Module: features.15.conv.2.mix_weight.conv, Input size: torch.Size([1, 816, 14, 14]), Output size: torch.Size([1, 136, 14, 14])\n",
      "Module: features.16.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 136, 14, 14]), Output size: torch.Size([1, 816, 14, 14])\n",
      "Module: features.16.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 816, 14, 14]), Output size: torch.Size([1, 816, 14, 14])\n",
      "Module: features.16.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 816, 14, 14]), Output size: torch.Size([1, 34, 14, 14])\n",
      "Module: features.16.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 34, 14, 14]), Output size: torch.Size([1, 816, 14, 14])\n",
      "Module: features.16.conv.2.mix_weight.conv, Input size: torch.Size([1, 816, 14, 14]), Output size: torch.Size([1, 136, 14, 14])\n",
      "Module: features.17.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 136, 14, 14]), Output size: torch.Size([1, 816, 14, 14])\n",
      "Module: features.17.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 816, 14, 14]), Output size: torch.Size([1, 816, 14, 14])\n",
      "Module: features.17.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 816, 14, 14]), Output size: torch.Size([1, 34, 14, 14])\n",
      "Module: features.17.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 34, 14, 14]), Output size: torch.Size([1, 816, 14, 14])\n",
      "Module: features.17.conv.2.mix_weight.conv, Input size: torch.Size([1, 816, 14, 14]), Output size: torch.Size([1, 136, 14, 14])\n",
      "Module: features.18.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 136, 14, 14]), Output size: torch.Size([1, 816, 14, 14])\n",
      "Module: features.18.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 816, 14, 14]), Output size: torch.Size([1, 816, 14, 14])\n",
      "Module: features.18.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 816, 14, 14]), Output size: torch.Size([1, 34, 14, 14])\n",
      "Module: features.18.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 34, 14, 14]), Output size: torch.Size([1, 816, 14, 14])\n",
      "Module: features.18.conv.2.mix_weight.conv, Input size: torch.Size([1, 816, 14, 14]), Output size: torch.Size([1, 136, 14, 14])\n",
      "Module: features.19.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 136, 14, 14]), Output size: torch.Size([1, 816, 14, 14])\n",
      "Module: features.19.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 816, 14, 14]), Output size: torch.Size([1, 816, 7, 7])\n",
      "Module: features.19.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 816, 7, 7]), Output size: torch.Size([1, 34, 7, 7])\n",
      "Module: features.19.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 34, 7, 7]), Output size: torch.Size([1, 816, 7, 7])\n",
      "Module: features.19.conv.2.mix_weight.conv, Input size: torch.Size([1, 816, 7, 7]), Output size: torch.Size([1, 232, 7, 7])\n",
      "Module: features.20.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 232, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.20.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.20.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 58, 7, 7])\n",
      "Module: features.20.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 58, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.20.conv.2.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 232, 7, 7])\n",
      "Module: features.21.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 232, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.21.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.21.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 58, 7, 7])\n",
      "Module: features.21.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 58, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.21.conv.2.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 232, 7, 7])\n",
      "Module: features.22.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 232, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.22.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.22.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 58, 7, 7])\n",
      "Module: features.22.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 58, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.22.conv.2.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 232, 7, 7])\n",
      "Module: features.23.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 232, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.23.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.23.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 58, 7, 7])\n",
      "Module: features.23.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 58, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.23.conv.2.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 232, 7, 7])\n",
      "Module: features.24.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 232, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.24.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.24.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 58, 7, 7])\n",
      "Module: features.24.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 58, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.24.conv.2.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 232, 7, 7])\n",
      "Module: features.25.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 232, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.25.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.25.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 58, 7, 7])\n",
      "Module: features.25.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 58, 7, 7]), Output size: torch.Size([1, 1392, 7, 7])\n",
      "Module: features.25.conv.2.mix_weight.conv, Input size: torch.Size([1, 1392, 7, 7]), Output size: torch.Size([1, 388, 7, 7])\n",
      "Module: features.26.expand_conv.cnn.mix_weight.conv, Input size: torch.Size([1, 388, 7, 7]), Output size: torch.Size([1, 2328, 7, 7])\n",
      "Module: features.26.conv.0.cnn.mix_weight.conv, Input size: torch.Size([1, 2328, 7, 7]), Output size: torch.Size([1, 2328, 7, 7])\n",
      "Module: features.26.conv.1.se.1.mix_weight.conv, Input size: torch.Size([1, 2328, 7, 7]), Output size: torch.Size([1, 97, 7, 7])\n",
      "Module: features.26.conv.1.se.3.mix_weight.conv, Input size: torch.Size([1, 97, 7, 7]), Output size: torch.Size([1, 2328, 7, 7])\n",
      "Module: features.26.conv.2.mix_weight.conv, Input size: torch.Size([1, 2328, 7, 7]), Output size: torch.Size([1, 388, 7, 7])\n",
      "Module: features.27.cnn.mix_weight.conv, Input size: torch.Size([1, 388, 7, 7]), Output size: torch.Size([1, 1549, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "def test_conv2d_input_sizes(model, input_size):\n",
    "    device = next(model.parameters()).device\n",
    "    mock_input = torch.randn(1, 3, input_size, input_size).to(device)\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            try:\n",
    "                # モック入力をモジュールに適用\n",
    "                output = module(mock_input)\n",
    "                print(f\"Module: {name}, Input size: {mock_input.shape}, Output size: {output.shape}\")\n",
    "                mock_input = output  # 次のモジュールのために出力を入力として更新\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error processing module {name}: {e}\")\n",
    "                break\n",
    "\n",
    "# モデルのインスタンス化\n",
    "# model = mixeffnet_b0_w1234a234()  # または他のバージョン\n",
    "\n",
    "# モジュールのテスト\n",
    "test_conv2d_input_sizes(model, input_size=224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abits: [2, 4, 6, 8]\n",
      "wbits: [2, 4, 6, 8]\n",
      "Error processing layer classifier.1: mat1 and mat2 shapes cannot be multiplied (1x75901 and 1549x100)\n",
      "{'layer_name': 'features.1.conv.1.se.3.mix_weight.conv', 'compression': 0.75, 'cnn_layer_number': 4}\n",
      "{'layer_name': 'features.2.conv.1.se.3.mix_weight.conv', 'compression': 0.4166666666666667, 'cnn_layer_number': 8}\n",
      "{'layer_name': 'features.3.conv.1.se.3.mix_weight.conv', 'compression': 0.10416666666666667, 'cnn_layer_number': 13}\n",
      "{'layer_name': 'features.6.conv.1.se.3.mix_weight.conv', 'compression': 0.041666666666666664, 'cnn_layer_number': 28}\n",
      "{'layer_name': 'features.9.conv.1.se.3.mix_weight.conv', 'compression': 0.015625, 'cnn_layer_number': 43}\n",
      "{'layer_name': 'features.19.conv.1.se.3.mix_weight.conv', 'compression': 0.011067708333333334, 'cnn_layer_number': 93}\n"
     ]
    }
   ],
   "source": [
    "model = mixeffnet_b3_w2468a2468_100()\n",
    "# model = mobilenetv3()\n",
    "# model = torchvision.models.mobilenet_v3_large()\n",
    "\n",
    "def get_natural_bottlenecks_pytorch(model, input_size, compressive_only=True):\n",
    "    natural_bottlenecks = []\n",
    "    best_compression = 1.0\n",
    "    cnn_count = 0  # CNNレイヤーのカウント\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    mock_input = torch.randn(1, 3, input_size, input_size).to(device)\n",
    "\n",
    "    previous_size = torch.prod(torch.tensor(mock_input.shape[1:])).item()\n",
    "\n",
    "    for name, layer in model.named_modules():\n",
    "        try:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                cnn_count += 1  # CNNレイヤーをカウント\n",
    "\n",
    "            if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "                if isinstance(layer, nn.Linear) and len(mock_input.shape) > 2:\n",
    "                    mock_input = torch.flatten(mock_input, 1)\n",
    "\n",
    "                output = layer(mock_input)\n",
    "                input_size_layer = previous_size\n",
    "                previous_size = torch.prod(torch.tensor(output.shape[1:])).item()\n",
    "\n",
    "                if input_size_layer < input_size * input_size * 3:\n",
    "                    compression = float(input_size_layer) / (input_size * input_size * 3)\n",
    "                    if not compressive_only or compression < best_compression:\n",
    "                        natural_bottlenecks.append({\n",
    "                            'layer_name': name,\n",
    "                            'compression': compression,\n",
    "                            'cnn_layer_number': cnn_count  # ここでCNNレイヤーの番号を記録\n",
    "                        })\n",
    "                        best_compression = compression\n",
    "\n",
    "                mock_input = output.detach()\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error processing layer {name}: {e}\")\n",
    "            break\n",
    "\n",
    "    return natural_bottlenecks\n",
    "\n",
    "# ボトルネックを取得\n",
    "bottlenecks = get_natural_bottlenecks_pytorch(model, input_size=224)\n",
    "for bottleneck in bottlenecks:\n",
    "    print(bottleneck)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MobileNetV3(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "          (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "          (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "          (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "          (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "          (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (block): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          (2): Hardswish()\n",
      "        )\n",
      "        (2): SqueezeExcitation(\n",
      "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "          (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (scale_activation): Hardsigmoid()\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): Conv2dNormActivation(\n",
      "      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=960, out_features=1280, bias=True)\n",
      "    (1): Hardswish()\n",
      "    (2): Dropout(p=0.2, inplace=True)\n",
      "    (3): Linear(in_features=1280, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n",
      "features Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "        (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2dNormActivation(\n",
      "        (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Conv2dNormActivation(\n",
      "        (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (4): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "        (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (5): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "        (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (6): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "        (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (7): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "        (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): Conv2dNormActivation(\n",
      "        (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (8): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "        (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): Conv2dNormActivation(\n",
      "        (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (9): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "        (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): Conv2dNormActivation(\n",
      "        (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (10): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "        (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): Conv2dNormActivation(\n",
      "        (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (11): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "        (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (12): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "        (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (13): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "        (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (14): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (15): InvertedResidual(\n",
      "    (block): Sequential(\n",
      "      (0): Conv2dNormActivation(\n",
      "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (1): Conv2dNormActivation(\n",
      "        (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "        (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "        (2): Hardswish()\n",
      "      )\n",
      "      (2): SqueezeExcitation(\n",
      "        (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (activation): ReLU()\n",
      "        (scale_activation): Hardsigmoid()\n",
      "      )\n",
      "      (3): Conv2dNormActivation(\n",
      "        (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (16): Conv2dNormActivation(\n",
      "    (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      ")\n",
      "features.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.0.0 Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "features.0.1 BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.0.2 Hardswish()\n",
      "features.1 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.1.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "    (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.1.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "  (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      ")\n",
      "features.1.block.0.0 Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
      "features.1.block.0.1 BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.1.block.0.2 ReLU(inplace=True)\n",
      "features.1.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.1.block.1.0 Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.1.block.1.1 BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.2 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "      (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Conv2dNormActivation(\n",
      "      (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.2.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "    (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Conv2dNormActivation(\n",
      "    (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.2.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      ")\n",
      "features.2.block.0.0 Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.2.block.0.1 BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.2.block.0.2 ReLU(inplace=True)\n",
      "features.2.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "  (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      ")\n",
      "features.2.block.1.0 Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
      "features.2.block.1.1 BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.2.block.1.2 ReLU(inplace=True)\n",
      "features.2.block.2 Conv2dNormActivation(\n",
      "  (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.2.block.2.0 Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.2.block.2.1 BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.3 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "      (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Conv2dNormActivation(\n",
      "      (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.3.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "    (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Conv2dNormActivation(\n",
      "    (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.3.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      ")\n",
      "features.3.block.0.0 Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.3.block.0.1 BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.3.block.0.2 ReLU(inplace=True)\n",
      "features.3.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "  (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      ")\n",
      "features.3.block.1.0 Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
      "features.3.block.1.1 BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.3.block.1.2 ReLU(inplace=True)\n",
      "features.3.block.2 Conv2dNormActivation(\n",
      "  (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.3.block.2.0 Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.3.block.2.1 BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.4 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "      (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.4.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "    (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation): ReLU()\n",
      "    (scale_activation): Hardsigmoid()\n",
      "  )\n",
      "  (3): Conv2dNormActivation(\n",
      "    (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.4.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      ")\n",
      "features.4.block.0.0 Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.4.block.0.1 BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.4.block.0.2 ReLU(inplace=True)\n",
      "features.4.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "  (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      ")\n",
      "features.4.block.1.0 Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
      "features.4.block.1.1 BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.4.block.1.2 ReLU(inplace=True)\n",
      "features.4.block.2 SqueezeExcitation(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      "  (scale_activation): Hardsigmoid()\n",
      ")\n",
      "features.4.block.2.avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "features.4.block.2.fc1 Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.4.block.2.fc2 Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.4.block.2.activation ReLU()\n",
      "features.4.block.2.scale_activation Hardsigmoid()\n",
      "features.4.block.3 Conv2dNormActivation(\n",
      "  (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.4.block.3.0 Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.4.block.3.1 BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.5 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "      (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.5.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "    (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation): ReLU()\n",
      "    (scale_activation): Hardsigmoid()\n",
      "  )\n",
      "  (3): Conv2dNormActivation(\n",
      "    (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.5.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      ")\n",
      "features.5.block.0.0 Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.5.block.0.1 BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.5.block.0.2 ReLU(inplace=True)\n",
      "features.5.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "  (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      ")\n",
      "features.5.block.1.0 Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "features.5.block.1.1 BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.5.block.1.2 ReLU(inplace=True)\n",
      "features.5.block.2 SqueezeExcitation(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      "  (scale_activation): Hardsigmoid()\n",
      ")\n",
      "features.5.block.2.avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "features.5.block.2.fc1 Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.5.block.2.fc2 Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.5.block.2.activation ReLU()\n",
      "features.5.block.2.scale_activation Hardsigmoid()\n",
      "features.5.block.3 Conv2dNormActivation(\n",
      "  (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.5.block.3.0 Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.5.block.3.1 BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.6 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "      (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.6.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "    (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation): ReLU()\n",
      "    (scale_activation): Hardsigmoid()\n",
      "  )\n",
      "  (3): Conv2dNormActivation(\n",
      "    (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.6.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      ")\n",
      "features.6.block.0.0 Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.6.block.0.1 BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.6.block.0.2 ReLU(inplace=True)\n",
      "features.6.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "  (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): ReLU(inplace=True)\n",
      ")\n",
      "features.6.block.1.0 Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
      "features.6.block.1.1 BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.6.block.1.2 ReLU(inplace=True)\n",
      "features.6.block.2 SqueezeExcitation(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      "  (scale_activation): Hardsigmoid()\n",
      ")\n",
      "features.6.block.2.avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "features.6.block.2.fc1 Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.6.block.2.fc2 Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.6.block.2.activation ReLU()\n",
      "features.6.block.2.scale_activation Hardsigmoid()\n",
      "features.6.block.3 Conv2dNormActivation(\n",
      "  (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.6.block.3.0 Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.6.block.3.1 BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.7 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "      (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): Conv2dNormActivation(\n",
      "      (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.7.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "    (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (2): Conv2dNormActivation(\n",
      "    (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.7.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.7.block.0.0 Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.7.block.0.1 BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.7.block.0.2 Hardswish()\n",
      "features.7.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "  (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.7.block.1.0 Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
      "features.7.block.1.1 BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.7.block.1.2 Hardswish()\n",
      "features.7.block.2 Conv2dNormActivation(\n",
      "  (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.7.block.2.0 Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.7.block.2.1 BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.8 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "      (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): Conv2dNormActivation(\n",
      "      (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.8.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "    (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (2): Conv2dNormActivation(\n",
      "    (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.8.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.8.block.0.0 Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.8.block.0.1 BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.8.block.0.2 Hardswish()\n",
      "features.8.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "  (1): BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.8.block.1.0 Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
      "features.8.block.1.1 BatchNorm2d(200, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.8.block.1.2 Hardswish()\n",
      "features.8.block.2 Conv2dNormActivation(\n",
      "  (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.8.block.2.0 Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.8.block.2.1 BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.9 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "      (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): Conv2dNormActivation(\n",
      "      (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.9.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "    (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (2): Conv2dNormActivation(\n",
      "    (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.9.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.9.block.0.0 Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.9.block.0.1 BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.9.block.0.2 Hardswish()\n",
      "features.9.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "  (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.9.block.1.0 Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "features.9.block.1.1 BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.9.block.1.2 Hardswish()\n",
      "features.9.block.2 Conv2dNormActivation(\n",
      "  (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.9.block.2.0 Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.9.block.2.1 BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.10 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "      (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): Conv2dNormActivation(\n",
      "      (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.10.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "    (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (2): Conv2dNormActivation(\n",
      "    (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.10.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.10.block.0.0 Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.10.block.0.1 BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.10.block.0.2 Hardswish()\n",
      "features.10.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "  (1): BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.10.block.1.0 Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
      "features.10.block.1.1 BatchNorm2d(184, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.10.block.1.2 Hardswish()\n",
      "features.10.block.2 Conv2dNormActivation(\n",
      "  (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.10.block.2.0 Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.10.block.2.1 BatchNorm2d(80, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.11 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "      (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.11.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "    (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (2): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation): ReLU()\n",
      "    (scale_activation): Hardsigmoid()\n",
      "  )\n",
      "  (3): Conv2dNormActivation(\n",
      "    (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.11.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.11.block.0.0 Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.11.block.0.1 BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.11.block.0.2 Hardswish()\n",
      "features.11.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "  (1): BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.11.block.1.0 Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
      "features.11.block.1.1 BatchNorm2d(480, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.11.block.1.2 Hardswish()\n",
      "features.11.block.2 SqueezeExcitation(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      "  (scale_activation): Hardsigmoid()\n",
      ")\n",
      "features.11.block.2.avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "features.11.block.2.fc1 Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.11.block.2.fc2 Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.11.block.2.activation ReLU()\n",
      "features.11.block.2.scale_activation Hardsigmoid()\n",
      "features.11.block.3 Conv2dNormActivation(\n",
      "  (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.11.block.3.0 Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.11.block.3.1 BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.12 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "      (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.12.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "    (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (2): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation): ReLU()\n",
      "    (scale_activation): Hardsigmoid()\n",
      "  )\n",
      "  (3): Conv2dNormActivation(\n",
      "    (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.12.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.12.block.0.0 Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.12.block.0.1 BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.12.block.0.2 Hardswish()\n",
      "features.12.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "  (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.12.block.1.0 Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
      "features.12.block.1.1 BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.12.block.1.2 Hardswish()\n",
      "features.12.block.2 SqueezeExcitation(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      "  (scale_activation): Hardsigmoid()\n",
      ")\n",
      "features.12.block.2.avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "features.12.block.2.fc1 Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.12.block.2.fc2 Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.12.block.2.activation ReLU()\n",
      "features.12.block.2.scale_activation Hardsigmoid()\n",
      "features.12.block.3 Conv2dNormActivation(\n",
      "  (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.12.block.3.0 Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.12.block.3.1 BatchNorm2d(112, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.13 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "      (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.13.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "    (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (2): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation): ReLU()\n",
      "    (scale_activation): Hardsigmoid()\n",
      "  )\n",
      "  (3): Conv2dNormActivation(\n",
      "    (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.13.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.13.block.0.0 Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.13.block.0.1 BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.13.block.0.2 Hardswish()\n",
      "features.13.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "  (1): BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.13.block.1.0 Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
      "features.13.block.1.1 BatchNorm2d(672, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.13.block.1.2 Hardswish()\n",
      "features.13.block.2 SqueezeExcitation(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      "  (scale_activation): Hardsigmoid()\n",
      ")\n",
      "features.13.block.2.avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "features.13.block.2.fc1 Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.13.block.2.fc2 Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.13.block.2.activation ReLU()\n",
      "features.13.block.2.scale_activation Hardsigmoid()\n",
      "features.13.block.3 Conv2dNormActivation(\n",
      "  (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.13.block.3.0 Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.13.block.3.1 BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.14 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "      (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.14.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "    (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (2): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation): ReLU()\n",
      "    (scale_activation): Hardsigmoid()\n",
      "  )\n",
      "  (3): Conv2dNormActivation(\n",
      "    (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.14.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.14.block.0.0 Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.14.block.0.1 BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.14.block.0.2 Hardswish()\n",
      "features.14.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "  (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.14.block.1.0 Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "features.14.block.1.1 BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.14.block.1.2 Hardswish()\n",
      "features.14.block.2 SqueezeExcitation(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      "  (scale_activation): Hardsigmoid()\n",
      ")\n",
      "features.14.block.2.avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "features.14.block.2.fc1 Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.14.block.2.fc2 Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.14.block.2.activation ReLU()\n",
      "features.14.block.2.scale_activation Hardsigmoid()\n",
      "features.14.block.3 Conv2dNormActivation(\n",
      "  (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.14.block.3.0 Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.14.block.3.1 BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.15 InvertedResidual(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (1): Conv2dNormActivation(\n",
      "      (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "      (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (2): Hardswish()\n",
      "    )\n",
      "    (2): SqueezeExcitation(\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (activation): ReLU()\n",
      "      (scale_activation): Hardsigmoid()\n",
      "    )\n",
      "    (3): Conv2dNormActivation(\n",
      "      (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features.15.block Sequential(\n",
      "  (0): Conv2dNormActivation(\n",
      "    (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (1): Conv2dNormActivation(\n",
      "    (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "    (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): Hardswish()\n",
      "  )\n",
      "  (2): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (activation): ReLU()\n",
      "    (scale_activation): Hardsigmoid()\n",
      "  )\n",
      "  (3): Conv2dNormActivation(\n",
      "    (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "features.15.block.0 Conv2dNormActivation(\n",
      "  (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.15.block.0.0 Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.15.block.0.1 BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.15.block.0.2 Hardswish()\n",
      "features.15.block.1 Conv2dNormActivation(\n",
      "  (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "  (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.15.block.1.0 Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
      "features.15.block.1.1 BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.15.block.1.2 Hardswish()\n",
      "features.15.block.2 SqueezeExcitation(\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      "  (scale_activation): Hardsigmoid()\n",
      ")\n",
      "features.15.block.2.avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "features.15.block.2.fc1 Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.15.block.2.fc2 Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
      "features.15.block.2.activation ReLU()\n",
      "features.15.block.2.scale_activation Hardsigmoid()\n",
      "features.15.block.3 Conv2dNormActivation(\n",
      "  (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      ")\n",
      "features.15.block.3.0 Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.15.block.3.1 BatchNorm2d(160, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.16 Conv2dNormActivation(\n",
      "  (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (1): BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  (2): Hardswish()\n",
      ")\n",
      "features.16.0 Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "features.16.1 BatchNorm2d(960, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "features.16.2 Hardswish()\n",
      "avgpool AdaptiveAvgPool2d(output_size=1)\n",
      "classifier Sequential(\n",
      "  (0): Linear(in_features=960, out_features=1280, bias=True)\n",
      "  (1): Hardswish()\n",
      "  (2): Dropout(p=0.2, inplace=True)\n",
      "  (3): Linear(in_features=1280, out_features=1000, bias=True)\n",
      ")\n",
      "classifier.0 Linear(in_features=960, out_features=1280, bias=True)\n",
      "classifier.1 Hardswish()\n",
      "classifier.2 Dropout(p=0.2, inplace=True)\n",
      "classifier.3 Linear(in_features=1280, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "# model = mixeffnet_b3_w2468a2468_100()\n",
    "# model = mobilenetv3()\n",
    "model = torchvision.models.mobilenet_v3_large()\n",
    "for name, layer in model.named_modules():\n",
    "    print(name, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abits: [2, 4, 6, 8]\n",
      "wbits: [2, 4, 6, 8]\n",
      "Error processing layer classifier.1: mat1 and mat2 shapes cannot be multiplied (1x62720 and 1280x100)\n",
      "{'layer_name': 'features.1.conv.1.se.3.mix_weight.conv', 'compression': 0.6666666666666666, 'cnn_layer_number': 4}\n",
      "{'layer_name': 'features.2.conv.1.se.3.mix_weight.conv', 'compression': 0.08333333333333333, 'cnn_layer_number': 9}\n",
      "{'layer_name': 'features.4.conv.1.se.3.mix_weight.conv', 'compression': 0.03125, 'cnn_layer_number': 19}\n",
      "{'layer_name': 'features.6.conv.1.se.3.mix_weight.conv', 'compression': 0.013020833333333334, 'cnn_layer_number': 29}\n",
      "{'layer_name': 'features.12.conv.1.se.3.mix_weight.conv', 'compression': 0.009114583333333334, 'cnn_layer_number': 59}\n"
     ]
    }
   ],
   "source": [
    "model = mixeffnet_b0_w2468a2468_100()\n",
    "\n",
    "# bitの出力を直接入力できるようにする、その後DSCを実行する\n",
    "def get_natural_bottlenecks_pytorch(model, input_size, compressive_only=True):\n",
    "    natural_bottlenecks = []\n",
    "    best_compression = 1.0\n",
    "    cnn_count = 0  # CNNレイヤーのカウント\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    mock_input = torch.randn(1, 3, input_size, input_size).to(device)\n",
    "\n",
    "    previous_size = torch.prod(torch.tensor(mock_input.shape[1:])).item()\n",
    "    for name, layer in model.named_modules():\n",
    "        try:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                cnn_count += 1  # CNNレイヤーをカウント\n",
    "\n",
    "            if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "                if isinstance(layer, nn.Linear) and len(mock_input.shape) > 2:\n",
    "                    mock_input = torch.flatten(mock_input, 1)\n",
    "\n",
    "                output = layer(mock_input)\n",
    "                input_size_layer = previous_size\n",
    "                previous_size = torch.prod(torch.tensor(output.shape[1:])).item()\n",
    "\n",
    "                if input_size_layer < input_size * input_size * 3:\n",
    "                    compression = float(input_size_layer) / (input_size * input_size * 3)\n",
    "                    if not compressive_only or compression < best_compression:\n",
    "                        natural_bottlenecks.append({\n",
    "                            'layer_name': name,\n",
    "                            'compression': compression,\n",
    "                            'cnn_layer_number': cnn_count  # ここでCNNレイヤーの番号を記録\n",
    "                        })\n",
    "                        best_compression = compression\n",
    "\n",
    "                mock_input = output.detach()\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error processing layer {name}: {e}\")\n",
    "            break\n",
    "\n",
    "    return natural_bottlenecks\n",
    "\n",
    "\n",
    "\n",
    "# ボトルネックを取得\n",
    "bottlenecks = get_natural_bottlenecks_pytorch(model, input_size=224)\n",
    "for bottleneck in bottlenecks:\n",
    "    print(bottleneck)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abits: [2, 4, 6, 8]\n",
      "wbits: [2, 4, 6, 8]\n",
      "Encountered BasicBlock at features.0\n",
      "{'layer_name': 'block_1', 'compression': 0.6666666666666666, 'cnn_layer_number': 0}\n",
      "{'layer_name': 'block_2', 'compression': 0.3333333333333333, 'cnn_layer_number': 4}\n",
      "{'layer_name': 'block_3', 'compression': 0.125, 'cnn_layer_number': 9}\n",
      "{'layer_name': 'block_5', 'compression': 0.052083333333333336, 'cnn_layer_number': 19}\n",
      "{'layer_name': 'block_7', 'compression': 0.026041666666666668, 'cnn_layer_number': 29}\n",
      "{'layer_name': 'block_13', 'compression': 0.015625, 'cnn_layer_number': 59}\n",
      "[0, 4, 9, 19, 29, 59]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "model = mixeffnet_b0_w2468a2468_100()\n",
    "\n",
    "\n",
    "def get_natural_bottlenecks_pytorch(model, input_size, compressive_only=True):\n",
    "    # 各層のinputサイズを計算して、圧縮率が最も高い層を探す\n",
    "    natural_bottlenecks = []\n",
    "    best_compression = 1.0\n",
    "    cnn_count = 0  # CNNレイヤーのカウント\n",
    "    input_bit = 8 # 入力のbit数\n",
    "    min_bit = 2  # 探索する最小のbit数\n",
    "    bit_compression = min_bit / input_bit\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    mock_input = torch.randn(1, 3, input_size, input_size).to(device)\n",
    "    previous_size = torch.prod(torch.tensor(mock_input.shape[1:])).item()\n",
    "\n",
    "    for i, module in enumerate(model.features):\n",
    "        # print(i, module)\n",
    "        block_number = i\n",
    "        if isinstance(module, BasicCNNBlock):\n",
    "            print(f\"Encountered BasicBlock at features.{i}\")\n",
    "            output = module(mock_input)\n",
    "            mock_input = output.detach()\n",
    "            continue\n",
    "        \n",
    "        input_size_layer = torch.prod(torch.tensor(mock_input.shape[1:])).item()\n",
    "        if input_size_layer * min_bit < input_size * input_size * 3 * input_bit:\n",
    "            compression = float(input_size_layer) / (input_size * input_size * 3)\n",
    "            compression *= bit_compression\n",
    "            if not compressive_only or compression < best_compression:\n",
    "                natural_bottlenecks.append({\n",
    "                    'layer_name': \"block_{}\".format(block_number),\n",
    "                    'compression': compression,\n",
    "                    'cnn_layer_number': cnn_count  # ここでCNNレイヤーの番号を記録\n",
    "                })\n",
    "                best_compression = compression\n",
    "        output = module(mock_input)\n",
    "        mock_input = output.detach()\n",
    "        \n",
    "        cnn_count += count_conv2d_layers(module)\n",
    "\n",
    "\n",
    "    return natural_bottlenecks\n",
    "\n",
    "def count_conv2d_layers(model):\n",
    "    count = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            count += 1\n",
    "        elif isinstance(module, nn.Sequential):\n",
    "            # Sequentialブロック内でさらにConv2dを探す\n",
    "            for sub_module in module:\n",
    "                if isinstance(sub_module, nn.Conv2d):\n",
    "                    count += 1\n",
    "    return count\n",
    "\n",
    "# ボトルネックを取得\n",
    "bottlenecks = get_natural_bottlenecks_pytorch(model, input_size=224)\n",
    "for bottleneck in bottlenecks:\n",
    "    print(bottleneck)\n",
    "    \n",
    "# cnn_layer_numberのみをリストに収集\n",
    "cnn_layer_numbers = [bottleneck['cnn_layer_number'] for bottleneck in bottlenecks]\n",
    "\n",
    "print(cnn_layer_numbers)\n",
    "print(len(cnn_layer_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abits: [2, 4, 6, 8]\n",
      "wbits: [2, 4, 6, 8]\n"
     ]
    }
   ],
   "source": [
    "summary(model=mixeffnet_b3_w2468a2468_100(), input_size=(1, 3, 224, 224))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (features): Sequential(\n",
       "    (0): BasicCNNBlock(\n",
       "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (1): InvertedResidualBlock(\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(16, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(24, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(24, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(40, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(40, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(80, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(80, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(80, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(112, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(112, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(112, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(192, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(192, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(192, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(192, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): CNNBlock(\n",
       "      (cnn): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (bn): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=100, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "model = torchvision.models.efficientnet_b3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "EfficientNet                                            [1, 1000]                 --\n",
       "├─Sequential: 1-1                                       [1, 1536, 7, 7]           --\n",
       "│    └─Conv2dNormActivation: 2-1                        [1, 40, 112, 112]         --\n",
       "│    │    └─Conv2d: 3-1                                 [1, 40, 112, 112]         1,080\n",
       "│    │    └─BatchNorm2d: 3-2                            [1, 40, 112, 112]         80\n",
       "│    │    └─SiLU: 3-3                                   [1, 40, 112, 112]         --\n",
       "│    └─Sequential: 2-2                                  [1, 24, 112, 112]         --\n",
       "│    │    └─MBConv: 3-4                                 [1, 24, 112, 112]         2,298\n",
       "│    │    └─MBConv: 3-5                                 [1, 24, 112, 112]         1,206\n",
       "│    └─Sequential: 2-3                                  [1, 32, 56, 56]           --\n",
       "│    │    └─MBConv: 3-6                                 [1, 32, 56, 56]           11,878\n",
       "│    │    └─MBConv: 3-7                                 [1, 32, 56, 56]           18,120\n",
       "│    │    └─MBConv: 3-8                                 [1, 32, 56, 56]           18,120\n",
       "│    └─Sequential: 2-4                                  [1, 48, 28, 28]           --\n",
       "│    │    └─MBConv: 3-9                                 [1, 48, 28, 28]           24,296\n",
       "│    │    └─MBConv: 3-10                                [1, 48, 28, 28]           43,308\n",
       "│    │    └─MBConv: 3-11                                [1, 48, 28, 28]           43,308\n",
       "│    └─Sequential: 2-5                                  [1, 96, 14, 14]           --\n",
       "│    │    └─MBConv: 3-12                                [1, 96, 14, 14]           52,620\n",
       "│    │    └─MBConv: 3-13                                [1, 96, 14, 14]           146,520\n",
       "│    │    └─MBConv: 3-14                                [1, 96, 14, 14]           146,520\n",
       "│    │    └─MBConv: 3-15                                [1, 96, 14, 14]           146,520\n",
       "│    │    └─MBConv: 3-16                                [1, 96, 14, 14]           146,520\n",
       "│    └─Sequential: 2-6                                  [1, 136, 14, 14]          --\n",
       "│    │    └─MBConv: 3-17                                [1, 136, 14, 14]          178,856\n",
       "│    │    └─MBConv: 3-18                                [1, 136, 14, 14]          302,226\n",
       "│    │    └─MBConv: 3-19                                [1, 136, 14, 14]          302,226\n",
       "│    │    └─MBConv: 3-20                                [1, 136, 14, 14]          302,226\n",
       "│    │    └─MBConv: 3-21                                [1, 136, 14, 14]          302,226\n",
       "│    └─Sequential: 2-7                                  [1, 232, 7, 7]            --\n",
       "│    │    └─MBConv: 3-22                                [1, 232, 7, 7]            380,754\n",
       "│    │    └─MBConv: 3-23                                [1, 232, 7, 7]            849,642\n",
       "│    │    └─MBConv: 3-24                                [1, 232, 7, 7]            849,642\n",
       "│    │    └─MBConv: 3-25                                [1, 232, 7, 7]            849,642\n",
       "│    │    └─MBConv: 3-26                                [1, 232, 7, 7]            849,642\n",
       "│    │    └─MBConv: 3-27                                [1, 232, 7, 7]            849,642\n",
       "│    └─Sequential: 2-8                                  [1, 384, 7, 7]            --\n",
       "│    │    └─MBConv: 3-28                                [1, 384, 7, 7]            1,039,258\n",
       "│    │    └─MBConv: 3-29                                [1, 384, 7, 7]            2,244,960\n",
       "│    └─Conv2dNormActivation: 2-9                        [1, 1536, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-30                                [1, 1536, 7, 7]           589,824\n",
       "│    │    └─BatchNorm2d: 3-31                           [1, 1536, 7, 7]           3,072\n",
       "│    │    └─SiLU: 3-32                                  [1, 1536, 7, 7]           --\n",
       "├─AdaptiveAvgPool2d: 1-2                                [1, 1536, 1, 1]           --\n",
       "├─Sequential: 1-3                                       [1, 1000]                 --\n",
       "│    └─Dropout: 2-10                                    [1, 1536]                 --\n",
       "│    └─Linear: 2-11                                     [1, 1000]                 1,537,000\n",
       "=========================================================================================================\n",
       "Total params: 12,233,232\n",
       "Trainable params: 12,233,232\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 962.82\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 207.95\n",
       "Params size (MB): 48.93\n",
       "Estimated Total Size (MB): 257.48\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=model, input_size=(1, 3, 224, 224))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.efficientnet_b0()\n",
    "model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layer_name': 'features', 'compression': 0.16666666666666666}\n",
      "{'layer_name': 'classifier', 'compression': 0.03985969387755102}\n"
     ]
    }
   ],
   "source": [
    "# resnet 分割点\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "def get_natural_bottlenecks_pytorch(model, input_size, compressive_only=True):\n",
    "    natural_bottlenecks = []\n",
    "    best_compression = 1.0\n",
    "    device = next(model.parameters()).device\n",
    "    mock_input = torch.randn(1, 3, input_size, input_size).to(device)\n",
    "\n",
    "    for name, layer in model.named_children():\n",
    "        # 処理する前にリシェイプを行う必要がある層をチェック\n",
    "        if isinstance(layer, nn.AdaptiveAvgPool2d):\n",
    "            # 平均プーリング層の出力をリシェイプ\n",
    "            mock_input = layer(mock_input)\n",
    "            mock_input = mock_input.view(mock_input.size(0), -1)\n",
    "        else:\n",
    "            try:\n",
    "                output = layer(mock_input)\n",
    "                input_size_layer = torch.prod(torch.tensor(mock_input.shape[1:])).item()\n",
    "                mock_input = output\n",
    "                output_size = torch.prod(torch.tensor(output.shape[1:])).item()\n",
    "\n",
    "                compression = float(output_size) / input_size_layer\n",
    "                if not compressive_only or compression < best_compression:\n",
    "                    natural_bottlenecks.append({\n",
    "                        'layer_name': name,\n",
    "                        'compression': compression\n",
    "                    })\n",
    "                    best_compression = compression\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error processing layer {name}: {e}\")\n",
    "                break\n",
    "\n",
    "    return natural_bottlenecks\n",
    "\n",
    "# ResNetモデルのインスタンス化\n",
    "model = models.vgg11()\n",
    "\n",
    "# ボトルネックの特定\n",
    "bottlenecks = get_natural_bottlenecks_pytorch(model, input_size=224)\n",
    "for bottleneck in bottlenecks:\n",
    "    print(bottleneck)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (pool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (features): Sequential(\n",
       "    (0): BasicCNNBlock(\n",
       "      (conv): Conv2d(3, 38, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(38, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (1): InvertedResidualBlock(\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(38, 38, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=38, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(38, 9, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(9, 38, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(38, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidualBlock(\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(20, 5, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(5, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(20, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(20, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(120, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=120, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(120, 5, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(5, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(32, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(32, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(32, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(192, 192, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=192, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(192, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(8, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(48, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(48, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=288, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(48, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(288, 288, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=288, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(288, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(12, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(96, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(96, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(96, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(96, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(96, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(576, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(24, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(576, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(136, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(136, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(136, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(136, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(816, 816, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=816, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (19): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(136, 816, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(816, 816, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=816, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(816, 34, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(34, 816, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(816, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (20): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(232, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (21): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(232, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (22): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(232, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (23): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(232, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (24): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(232, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(1392, 1392, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1392, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(232, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (25): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(232, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(1392, 1392, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1392, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(1392, 58, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(58, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(1392, 388, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(388, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (26): InvertedResidualBlock(\n",
       "      (expand_conv): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(388, 2328, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(2328, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (conv): Sequential(\n",
       "        (0): CNNBlock(\n",
       "          (cnn): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(2328, 2328, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2328, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (bn): BatchNorm2d(2328, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (silu): SiLU()\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (se): Sequential(\n",
       "            (0): AdaptiveAvgPool2d(output_size=1)\n",
       "            (1): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(2328, 97, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SiLU()\n",
       "            (3): MixActivConv2d(\n",
       "              (mix_activ): MixQuantActiv(\n",
       "                (mix_activ): ModuleList(\n",
       "                  (0): HWGQ()\n",
       "                  (1): HWGQ()\n",
       "                  (2): HWGQ()\n",
       "                  (3): HWGQ()\n",
       "                )\n",
       "              )\n",
       "              (mix_weight): SharedMixQuantConv2d(\n",
       "                (conv): Conv2d(97, 2328, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              )\n",
       "            )\n",
       "            (4): Sigmoid()\n",
       "          )\n",
       "        )\n",
       "        (2): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(2328, 388, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BatchNorm2d(388, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (27): CNNBlock(\n",
       "      (cnn): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(388, 1549, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (bn): BatchNorm2d(1549, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.3, inplace=False)\n",
       "    (1): Linear(in_features=1549, out_features=100, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EfficientNet' object has no attribute 'avgpool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavgpool\u001b[49m\u001b[38;5;241m.\u001b[39mchildren())[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EfficientNet' object has no attribute 'avgpool'"
     ]
    }
   ],
   "source": [
    "nn.Sequential(*list(model.avgpool.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Inference Time: 10.895565032958984 seconds\n",
      "Edge Inference Time: 10.031911134719849 seconds\n",
      "Data Transfer Time: 0.00036787986755371094 seconds\n",
      "Server Inference Time: 0.863286018371582 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "\n",
    "\n",
    "# GPUの存在を確認\n",
    "if torch.cuda.device_count() < 2:\n",
    "    raise RuntimeError(\"このコードは少なくとも2つのGPUが必要です。\")\n",
    "\n",
    "def measure_inference_time(model, input_data, device):\n",
    "    start_time = time.time()\n",
    "    model(input_data.to(device))\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "\n",
    "# 全体モデルの定義\n",
    "class FullModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullModel, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 56 * 56, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "def run_experiment(edge_model, server_model, input_data):\n",
    "    # エッジ側での推論時間を計測\n",
    "    edge_inference_time = measure_inference_time(edge_model, input_data, 'cuda:0')\n",
    "    edge_output = edge_model(input_data)\n",
    "\n",
    "    # データ転送時間を計測\n",
    "    start_transfer_time = time.time()\n",
    "    edge_output = edge_output.to('cuda:1')\n",
    "    end_transfer_time = time.time()\n",
    "    data_transfer_time = end_transfer_time - start_transfer_time\n",
    "\n",
    "    # サーバー側での推論時間を計測\n",
    "    server_inference_time = measure_inference_time(server_model, edge_output, 'cuda:1')\n",
    "\n",
    "    # 合計時間を計算\n",
    "    total_time = edge_inference_time + data_transfer_time + server_inference_time\n",
    "    return total_time, edge_inference_time, data_transfer_time, server_inference_time\n",
    "\n",
    "def main():\n",
    "    # モデルのインスタンス化と分割\n",
    "    full_model = FullModel()\n",
    "    split_point = 4\n",
    "    edge_model = nn.Sequential(*list(full_model.layers[:split_point])).to('cuda:0')\n",
    "    server_model = nn.Sequential(*list(full_model.layers[split_point:])).to('cuda:1')\n",
    "\n",
    "    # ダミーの入力データ\n",
    "    input_data = torch.randn(1, 3, 224, 224).to('cuda:0')\n",
    "\n",
    "    # 実験の実行\n",
    "    total_time, edge_inference_time, data_transfer_time, server_inference_time = run_experiment(edge_model, server_model, input_data)\n",
    "\n",
    "    # 結果の表示\n",
    "    print(f'Total Inference Time: {total_time} seconds')\n",
    "    print(f'Edge Inference Time: {edge_inference_time} seconds')\n",
    "    print(f'Data Transfer Time: {data_transfer_time} seconds')\n",
    "    print(f'Server Inference Time: {server_inference_time} seconds')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in model.named_children():\n",
    "#     print(name)\n",
    "    \n",
    "# # モデルのすべてのモジュールを列挙\n",
    "# for name, module in model.named_modules():\n",
    "#     print(name)\n",
    "\n",
    "# for child in model.children():\n",
    "#     print(child)\n",
    "\n",
    "children = list(model.children())  #どんなモデルにも対応させる場合\n",
    "\n",
    "# split_point = bottlenecks[0]['layer_index'] if bottlenecks else len(list(model.children()))\n",
    "\n",
    "# モデルの分割\n",
    "# children = list(model.children())\n",
    "# edge_model = nn.Sequential(*children[:split_point]).to('cuda:0')\n",
    "# server_model = nn.Sequential(*children[split_point:]).to('cuda:1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abits: [2, 4, 6, 8]\n",
      "wbits: [2, 4, 6, 8]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): BasicCNNBlock(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (silu): SiLU()\n",
       "  )\n",
       "  (1): InvertedResidualBlock(\n",
       "    (conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (1): SqueezeExcitation(\n",
       "        (se): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SiLU()\n",
       "          (3): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (2): InvertedResidualBlock(\n",
       "    (expand_conv): CNNBlock(\n",
       "      (cnn): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(16, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (1): SqueezeExcitation(\n",
       "        (se): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SiLU()\n",
       "          (3): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (3): InvertedResidualBlock(\n",
       "    (expand_conv): CNNBlock(\n",
       "      (cnn): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(24, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (1): SqueezeExcitation(\n",
       "        (se): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SiLU()\n",
       "          (3): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (4): InvertedResidualBlock(\n",
       "    (expand_conv): CNNBlock(\n",
       "      (cnn): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(24, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (1): SqueezeExcitation(\n",
       "        (se): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SiLU()\n",
       "          (3): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): InvertedResidualBlock(\n",
       "    (expand_conv): CNNBlock(\n",
       "      (cnn): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(40, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (1): SqueezeExcitation(\n",
       "        (se): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SiLU()\n",
       "          (3): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): InvertedResidualBlock(\n",
       "    (expand_conv): CNNBlock(\n",
       "      (cnn): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(40, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (1): SqueezeExcitation(\n",
       "        (se): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SiLU()\n",
       "          (3): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): InvertedResidualBlock(\n",
       "    (expand_conv): CNNBlock(\n",
       "      (cnn): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(80, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (1): SqueezeExcitation(\n",
       "        (se): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SiLU()\n",
       "          (3): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (8): InvertedResidualBlock(\n",
       "    (expand_conv): CNNBlock(\n",
       "      (cnn): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(80, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (1): SqueezeExcitation(\n",
       "        (se): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SiLU()\n",
       "          (3): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (9): InvertedResidualBlock(\n",
       "    (expand_conv): CNNBlock(\n",
       "      (cnn): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(80, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (1): SqueezeExcitation(\n",
       "        (se): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SiLU()\n",
       "          (3): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (10): InvertedResidualBlock(\n",
       "    (expand_conv): CNNBlock(\n",
       "      (cnn): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(112, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (1): SqueezeExcitation(\n",
       "        (se): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SiLU()\n",
       "          (3): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (11): InvertedResidualBlock(\n",
       "    (expand_conv): CNNBlock(\n",
       "      (cnn): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(112, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (1): SqueezeExcitation(\n",
       "        (se): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SiLU()\n",
       "          (3): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (12): InvertedResidualBlock(\n",
       "    (expand_conv): CNNBlock(\n",
       "      (cnn): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(112, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (1): SqueezeExcitation(\n",
       "        (se): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SiLU()\n",
       "          (3): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (13): InvertedResidualBlock(\n",
       "    (expand_conv): CNNBlock(\n",
       "      (cnn): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(192, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (1): SqueezeExcitation(\n",
       "        (se): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SiLU()\n",
       "          (3): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (14): InvertedResidualBlock(\n",
       "    (expand_conv): CNNBlock(\n",
       "      (cnn): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(192, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (1): SqueezeExcitation(\n",
       "        (se): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SiLU()\n",
       "          (3): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (15): InvertedResidualBlock(\n",
       "    (expand_conv): CNNBlock(\n",
       "      (cnn): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(192, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (1): SqueezeExcitation(\n",
       "        (se): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SiLU()\n",
       "          (3): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (16): InvertedResidualBlock(\n",
       "    (expand_conv): CNNBlock(\n",
       "      (cnn): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(192, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (conv): Sequential(\n",
       "      (0): CNNBlock(\n",
       "        (cnn): MixActivConv2d(\n",
       "          (mix_activ): MixQuantActiv(\n",
       "            (mix_activ): ModuleList(\n",
       "              (0): HWGQ()\n",
       "              (1): HWGQ()\n",
       "              (2): HWGQ()\n",
       "              (3): HWGQ()\n",
       "            )\n",
       "          )\n",
       "          (mix_weight): SharedMixQuantConv2d(\n",
       "            (conv): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (bn): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (1): SqueezeExcitation(\n",
       "        (se): Sequential(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): SiLU()\n",
       "          (3): MixActivConv2d(\n",
       "            (mix_activ): MixQuantActiv(\n",
       "              (mix_activ): ModuleList(\n",
       "                (0): HWGQ()\n",
       "                (1): HWGQ()\n",
       "                (2): HWGQ()\n",
       "                (3): HWGQ()\n",
       "              )\n",
       "            )\n",
       "            (mix_weight): SharedMixQuantConv2d(\n",
       "              (conv): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (2): MixActivConv2d(\n",
       "        (mix_activ): MixQuantActiv(\n",
       "          (mix_activ): ModuleList(\n",
       "            (0): HWGQ()\n",
       "            (1): HWGQ()\n",
       "            (2): HWGQ()\n",
       "            (3): HWGQ()\n",
       "          )\n",
       "        )\n",
       "        (mix_weight): SharedMixQuantConv2d(\n",
       "          (conv): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (17): CNNBlock(\n",
       "    (cnn): MixActivConv2d(\n",
       "      (mix_activ): MixQuantActiv(\n",
       "        (mix_activ): ModuleList(\n",
       "          (0): HWGQ()\n",
       "          (1): HWGQ()\n",
       "          (2): HWGQ()\n",
       "          (3): HWGQ()\n",
       "        )\n",
       "      )\n",
       "      (mix_weight): SharedMixQuantConv2d(\n",
       "        (conv): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (bn): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (silu): SiLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mixeffnet_b0_w2468a2468_100()\n",
    "model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abits: [2, 4, 6, 8]\n",
      "wbits: [2, 4, 6, 8]\n",
      "Encountered BasicBlock at features.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 46\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServer Inference Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mserver_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m input_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\u001b[38;5;241m.\u001b[39mto(edge_device)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 以降、エッジとサーバーでの推論処理を実装\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# エッジ側での推論\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m start_time \u001b[38;5;241m=\u001b[39m \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     24\u001b[0m edge_output \u001b[38;5;241m=\u001b[39m edge_model(input_data)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEdge output shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00medge_output\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# 出力形状の印刷\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "def split_efficientnet_model(model, split_layer, edge_device, server_device):\n",
    "    edge_model = nn.Sequential(*list(model.features[:split_layer])).to(edge_device)\n",
    "    server_model_layers = list(model.features[split_layer:]) + [model.pool] +[nn.Flatten()]\n",
    "    server_model_layers += list(model.classifier)\n",
    "    server_model = nn.Sequential(*server_model_layers).to(server_device)\n",
    "    return edge_model, server_model\n",
    "\n",
    "def main():\n",
    "    # モデルのインスタンス化\n",
    "    model = mixeffnet_b0_w2468a2468_100()\n",
    "\n",
    "    # ボトルネック層を取得\n",
    "    bottlenecks = get_natural_bottlenecks_pytorch(model, input_size=224)\n",
    "    #split_layer = bottlenecks[0]['cnn_layer_number']  # 最初のボトルネック層\n",
    "    split_layer = 4\n",
    "    # モデルを分割\n",
    "    edge_device = torch.device(\"cuda:0\")\n",
    "    server_device = torch.device(\"cuda:1\")\n",
    "    edge_model, server_model = split_efficientnet_model(model, split_layer, edge_device, server_device)\n",
    "    input_data = torch.randn(1, 3, 224, 224).to(edge_device)\n",
    "    # 以降、エッジとサーバーでの推論処理を実装\n",
    "    # エッジ側での推論\n",
    "    start_time = time.time()\n",
    "    edge_output = edge_model(input_data)\n",
    "    print(f\"Edge output shape: {edge_output.shape}\")  # 出力形状の印刷\n",
    "    edge_time = time.time() - start_time\n",
    "    \n",
    "    # エッジからサーバーへのデータ転送\n",
    "    start_time = time.time()\n",
    "    edge_output = edge_output.to(server_device)\n",
    "    transfer_time = time.time() - start_time\n",
    "    \n",
    "    # サーバー側での推論\n",
    "    start_time = time.time()\n",
    "    server_output = server_model(edge_output)\n",
    "    server_time = time.time() - start_time\n",
    "    \n",
    "    total_time = edge_time + transfer_time + server_time\n",
    "    print(f\"Total Inference Time: {total_time} seconds\")\n",
    "    print(f\"Edge Inference Time: {edge_time} seconds\")\n",
    "    print(f\"Data Transfer Time: {transfer_time} seconds\")\n",
    "    print(f\"Server Inference Time: {server_time} seconds\")\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archas: [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "archws: [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "23.305328699061647\n"
     ]
    }
   ],
   "source": [
    "def get_inference_time(model, batch_size, device, repetitions=100, input_shape=None, intermediate=None,):\n",
    "    \n",
    "    \"\"\"\n",
    "    Get the inference time of a model for a given input shape and batch size.\n",
    "    \"\"\"\n",
    "    # モデルを評価モードに設定し、適切なデバイスに移動\n",
    "    model = model.eval().to(device)\n",
    "\n",
    "    if input_shape is None:\n",
    "        input_shape = (batch_size, 3, config['image_size'], config['image_size'])\n",
    "    else:\n",
    "        input_shape = (batch_size,) + input_shape[1:]\n",
    "    \n",
    "    if intermediate is None:\n",
    "        input_data = torch.ones(input_shape, device=device)  # the original code uses dtype=torch.float16, which would be 2 bytes\n",
    "    else:\n",
    "        input_data = intermediate\n",
    "    # input_data = torch.randn(1, 3, 224, 224).to(device)\n",
    "    # ウォームアップフェーズ\n",
    "    with torch.no_grad():\n",
    "        for _ in range(repetitions):\n",
    "            model(input_data)\n",
    "            \n",
    "    # CUDAカーネルの同期化        \n",
    "    torch.cuda.synchronize(device)  # Make sure all CUDA operations have finished\n",
    "\n",
    "    # 推論時間の計測開始\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(repetitions):\n",
    "            model(input_data)\n",
    "            \n",
    "    # 再度、CUDAカーネルの同期化\n",
    "    torch.cuda.synchronize(device)\n",
    "    end = time.perf_counter()\n",
    "    \n",
    "    # 平均推論時間をミリ秒単位で計算\n",
    "    inference_time = (end - start) / repetitions * 100  # in milliseconds\n",
    "    return inference_time\n",
    "# model = mixeffnet_b0_w2468a2468_100()\n",
    "model = quanteffnet_w8a8(\"aaa\")\n",
    "# model = models.efficientnet_b0(pretrained=True)\n",
    "# model = quanteffnet_cfg_2468(\"/DynamicSplit/arch_model_best.pth.tar\")\n",
    "print(get_inference_time(model, 30, 'cuda:1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'processors': {\n",
    "        'weak': 'cuda:1',\n",
    "        'strong': 'cuda:0',\n",
    "    },\n",
    "    'model_name': 'efficientnet-b0',\n",
    "    'image_size': 224,\n",
    "    'batch_sizes': list(range(1, 2)),\n",
    "    'max_bandwidth': 128 * 10 ** 6,  # Bytes per second\n",
    "    'min_bandwidth': 1 * 10 ** 6,  # Bytes per second\n",
    "    'bandwidth_step': 1 * 10 ** 6,  # Bytes per second\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
